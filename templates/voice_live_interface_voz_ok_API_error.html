aNotepad aNotepad
- free online notepad
Workspace
Settings
Logout
voice_live_interface.html
Share Bookmark Save Copy
<!DOCTYPE html>
<html lang="es-AR">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>YPF Voice Live + Sistema minipywo - PRODUCTION FIXED</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #0a1428 0%, #1a2744 50%, #2a3754 100%);
            color: white;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            padding: 20px;
        }

        .container {
            text-align: center;
            max-width: 1000px;
            width: 100%;
        }

        .header {
            margin-bottom: 40px;
        }

        .ypf-logo {
            background: linear-gradient(135deg, #007AFF 0%, #0056CC 100%);
            color: white;
            padding: 16px 32px;
            border-radius: 16px;
            font-weight: 800;
            font-size: 32px;
            letter-spacing: 3px;
            margin-bottom: 20px;
            display: inline-block;
            box-shadow: 0 8px 32px rgba(0, 122, 255, 0.4);
        }

        .title {
            font-size: 28px;
            margin-bottom: 12px;
            font-weight: 600;
        }

        .subtitle {
            color: #FF6B35;
            font-weight: 600;
            font-size: 18px;
            margin-bottom: 10px;
        }

        .improvements {
            background: rgba(0, 255, 100, 0.1);
            border: 2px solid rgba(0, 255, 100, 0.3);
            border-radius: 12px;
            padding: 15px;
            margin-bottom: 15px;
            text-align: left;
        }

        .improvements h3 {
            color: #00FF64;
            margin-bottom: 10px;
            font-size: 16px;
        }

        .improvements ul {
            font-size: 14px;
            color: #B0C4DE;
            line-height: 1.4;
        }

        .improvements li {
            margin-bottom: 5px;
        }

        .description {
            color: #B0C4DE;
            font-size: 16px;
            max-width: 700px;
            margin: 0 auto;
        }

        .main-interface {
            display: flex;
            gap: 40px;
            align-items: flex-start;
            justify-content: center;
            flex-wrap: wrap;
            margin: 40px 0;
        }

        .avatar-section {
            display: flex;
            flex-direction: column;
            align-items: center;
        }

        .avatar-container {
            width: 320px;
            height: 320px;
            border-radius: 24px;
            background: linear-gradient(135deg, #1a2744 0%, #2a3754 100%);
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 100px;
            margin-bottom: 30px;
            box-shadow: 0 0 40px rgba(0, 122, 255, 0.3);
            transition: all 0.3s ease;
            border: 3px solid rgba(255, 255, 255, 0.1);
        }

        .avatar-container.listening {
            animation: pulse-listening 1.5s infinite;
            box-shadow: 0 0 60px rgba(0, 255, 100, 0.6);
            border-color: rgba(0, 255, 100, 0.5);
        }

        .avatar-container.thinking {
            animation: pulse-thinking 0.8s infinite;
            box-shadow: 0 0 60px rgba(255, 193, 7, 0.6);
            border-color: rgba(255, 193, 7, 0.5);
        }

        .avatar-container.speaking {
            animation: pulse-speaking 1s infinite;
            box-shadow: 0 0 60px rgba(255, 107, 53, 0.8);
            border-color: rgba(255, 107, 53, 0.5);
        }

        .avatar-container.vad-active {
            animation: pulse-vad 0.6s infinite;
            box-shadow: 0 0 80px rgba(255, 20, 147, 0.8);
            border-color: rgba(255, 20, 147, 0.6);
        }

        @keyframes pulse-listening {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.02); }
        }

        @keyframes pulse-thinking {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.03); }
        }

        @keyframes pulse-speaking {
            0%, 100% { transform: scale(1); }
            25% { transform: scale(1.05); }
            75% { transform: scale(1.02); }
        }

        @keyframes pulse-vad {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.06); }
        }

        .avatar-video-container {
            width: 320px; 
            height: 320px; 
            border-radius: 24px; 
            overflow: hidden; 
            margin-bottom: 20px; 
            display: none; 
            box-shadow: 0 0 40px rgba(255, 107, 53, 0.4);
        }

        .control-section {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 20px;
        }

        .control-button {
            width: 120px;
            height: 120px;
            border-radius: 50%;
            border: none;
            cursor: pointer;
            font-size: 48px;
            background: linear-gradient(135deg, #00FF64 0%, #00CC51 100%);
            color: white;
            transition: all 0.15s ease;
            box-shadow: 0 12px 40px rgba(0, 0, 0, 0.3);
            position: relative;
            overflow: hidden;
        }

        .control-button:hover {
            transform: scale(1.05);
            box-shadow: 0 16px 48px rgba(0, 0, 0, 0.4);
        }

        .control-button:active {
            transform: scale(0.95);
        }

        .control-button.active {
            background: linear-gradient(135deg, #FF3B30 0%, #CC2E24 100%);
            animation: button-active 1s infinite;
        }

        @keyframes button-active {
            0%, 100% { box-shadow: 0 12px 40px rgba(255, 59, 48, 0.4); }
            50% { box-shadow: 0 16px 48px rgba(255, 59, 48, 0.6); }
        }

        .status {
            margin: 20px 0;
            font-weight: 600;
            color: #00FF64;
            font-size: 20px;
            min-height: 30px;
        }

        .vad-info {
            background: rgba(255, 20, 147, 0.1);
            border: 1px solid rgba(255, 20, 147, 0.3);
            border-radius: 10px;
            padding: 12px;
            margin: 15px 0;
            font-size: 14px;
            color: #FFB6C1;
        }

        .conversation-section {
            max-width: 900px;
            margin: 30px auto;
        }

        .conversation-header {
            font-size: 20px;
            font-weight: 600;
            margin-bottom: 15px;
            color: #B0C4DE;
        }

        .conversation {
            max-height: 400px;
            overflow-y: auto;
            text-align: left;
            background: rgba(255, 255, 255, 0.05);
            border-radius: 20px;
            padding: 20px;
            backdrop-filter: blur(10px);
            border: 1px solid rgba(255, 255, 255, 0.1);
        }

        .message {
            margin: 15px 0;
            padding: 15px 20px;
            border-radius: 20px;
            max-width: 85%;
            word-wrap: break-word;
        }

        .message.user {
            background: linear-gradient(135deg, rgba(0, 122, 255, 0.3) 0%, rgba(0, 122, 255, 0.1) 100%);
            margin-left: auto;
            margin-right: 0;
            border-bottom-right-radius: 5px;
        }

        .message.assistant {
            background: linear-gradient(135deg, rgba(255, 107, 53, 0.3) 0%, rgba(255, 107, 53, 0.1) 100%);
            margin-right: auto;
            margin-left: 0;
            border-bottom-left-radius: 5px;
        }

        .message.system {
            background: linear-gradient(135deg, rgba(255, 193, 7, 0.3) 0%, rgba(255, 193, 7, 0.1) 100%);
            margin: 0 auto;
            border-radius: 15px;
            font-size: 14px;
            max-width: 90%;
        }

        .message strong {
            color: #FFD700;
            display: block;
            margin-bottom: 5px;
            font-size: 14px;
            font-weight: 700;
        }

        .tech-info {
            margin-top: 40px;
            padding: 25px;
            background: rgba(255, 255, 255, 0.05);
            border-radius: 15px;
            font-size: 14px;
            color: #B0C4DE;
            backdrop-filter: blur(10px);
        }

        .tech-badges {
            display: flex;
            gap: 10px;
            justify-content: center;
            flex-wrap: wrap;
            margin-top: 15px;
        }

        .tech-badge {
            background: rgba(0, 122, 255, 0.2);
            color: #87CEEB;
            padding: 5px 12px;
            border-radius: 20px;
            font-size: 12px;
            font-weight: 600;
        }

        .tech-badge.new {
            background: rgba(0, 255, 100, 0.2);
            color: #90EE90;
        }

        .error-message {
            color: #FF6B6B;
            background: rgba(255, 107, 107, 0.1);
            padding: 15px;
            border-radius: 10px;
            margin: 20px 0;
            border-left: 4px solid #FF6B6B;
        }

        .debug-panel {
            position: fixed;
            top: 20px;
            right: 20px;
            background: rgba(0, 0, 0, 0.9);
            padding: 15px;
            border-radius: 10px;
            font-size: 12px;
            color: #90EE90;
            max-width: 400px;
            max-height: 500px;
            overflow-y: auto;
            display: none;
            border: 1px solid rgba(0, 255, 100, 0.3);
        }

        .debug-panel.show {
            display: block;
        }

        .debug-panel h4 {
            color: #00FF64;
            margin-bottom: 10px;
            border-bottom: 1px solid rgba(0, 255, 100, 0.2);
            padding-bottom: 5px;
        }

        #clientId {
            display: none;
        }

        @media (max-width: 768px) {
            .main-interface {
                flex-direction: column;
                gap: 20px;
            }

            .avatar-container {
                width: 280px;
                height: 280px;
                font-size: 80px;
            }

            .control-button {
                width: 100px;
                height: 100px;
                font-size: 40px;
            }

            .ypf-logo {
                font-size: 24px;
                padding: 12px 24px;
            }

            .title {
                font-size: 24px;
            }

            .debug-panel {
                position: relative;
                top: auto;
                right: auto;
                margin: 20px 0;
            }
        }
    </style>
</head>
<body>
    <input type="hidden" id="clientId" value="{{ client_id }}">

    <div class="container">
        <div class="header">
            <div class="ypf-logo">YPF üá¶üá∑</div>
            <div class="title">Azure OpenAI Realtime + minipywo</div>
            <div class="subtitle">Production-Ready with Avatar Support</div>
            
            <div class="improvements">
                <h3>FIXED ISSUES:</h3>
                <ul>
                    <li><strong>WebSocket URL:</strong> Corrected to use /openai/realtime endpoint</li>
                    <li><strong>Audio Resampling:</strong> Proper 24kHz conversion implemented</li>
                    <li><strong>Voice Configuration:</strong> Using Azure OpenAI compatible voices</li>
                    <li><strong>Avatar WebRTC:</strong> Fixed Promise scope and SDP handling</li>
                    <li><strong>Session Timing:</strong> Audio starts after session.updated</li>
                </ul>
            </div>
            
            <div class="description">
                Production implementation with Azure OpenAI Realtime API for YPF operations.
                Optimized for Argentinian Spanish with proper audio resampling and avatar support.
            </div>
        </div>

        <div class="main-interface">
            <div class="avatar-section">
                <div class="avatar-container" id="avatar">üé≠</div>
                <div class="avatar-video-container" id="avatarVideoContainer">
                    <video id="avatarVideo" autoplay muted style="width: 100%; height: 100%; object-fit: cover;"></video>
                </div>
                <div class="status" id="status">System ready - Click to start</div>
                <div class="vad-info" id="vadInfo" style="display: none;">
                    VAD: <span id="vadThreshold">0.5</span> | 
                    Silence: <span id="vadSilence">500ms</span> | 
                    Signal: <span id="audioQuality">--</span> |
                    Avatar: <span id="avatarStatus">--</span>
                </div>
            </div>

            <div class="control-section">
                <button class="control-button" id="controlButton" onclick="toggleVoiceLive()">üéôÔ∏è</button>
                <div style="color: #B0C4DE; font-size: 14px; max-width: 220px; text-align: center;">
                    Press to talk with Tomas - YPF Assistant
                </div>
                <button onclick="toggleDebugPanel()" style="padding: 8px 16px; background: rgba(255,255,255,0.1); border: none; color: white; border-radius: 8px; cursor: pointer; font-size: 12px;">
                    Debug Console
                </button>
            </div>
        </div>

        <div class="conversation-section">
            <div class="conversation-header">Conversation Log</div>
            <div class="conversation" id="conversation">
                <div class="message system">
                    <strong>System:</strong>
                    Azure OpenAI Realtime API ready with proper audio resampling, 
                    VAD, and avatar support.
                </div>
                <div class="message assistant">
                    <strong>Tomas (YPF):</strong>
                    ¬°Che! Soy Tomas, tu asistente de YPF. Puedo ayudarte con informaci√≥n sobre equipos, 
                    pozos, workover y cualquier dato t√©cnico que necesites. ¬°Dale, preguntame lo que quieras!
                </div>
            </div>
        </div>

        <div class="tech-info">
            <strong>Technical Implementation:</strong><br>
            Azure OpenAI Realtime API with proper 24kHz audio resampling, server-side VAD,
            function calling integration, and WebRTC avatar support.
            
            <div class="tech-badges">
                <span class="tech-badge new">Azure OpenAI Realtime</span>
                <span class="tech-badge new">24kHz Resampling</span>
                <span class="tech-badge">Server VAD</span>
                <span class="tech-badge">Function Calling</span>
                <span class="tech-badge new">WebRTC Avatar</span>
                <span class="tech-badge">minipywo API</span>
            </div>
        </div>
    </div>

    <!-- Enhanced Debug Panel -->
    <div class="debug-panel" id="debugPanel">
        <h4>Debug Console - Azure OpenAI Realtime</h4>
        <div id="debugLog"></div>
    </div>

    <script src="https://cdn.socket.io/4.5.4/socket.io.min.js"></script>
    
    <script>
        // ================================
        // GLOBAL VARIABLES AND CONFIGURATION
        // ================================
        
        // Core WebSocket and Audio
        let voiceLiveWebSocket = null;
        let voiceLiveActive = false;
        let audioContext = null;
        let mediaStream = null;
        let audioProcessor = null;
        let audioSource = null;
        
        // Configuration
        let clientId = document.getElementById('clientId').value;
        let voiceLiveConfig = null;
        
        // Voice Activity Detection
        let vadActive = false;
        let speechStartTime = null;
        let speechEndTime = null;
        let currentTranscription = "";
        let finalTranscription = "";
        
        // Audio Processing
        let audioChunkQueue = [];
        let isPlayingAudio = false;
        let audioQualityMetrics = { 
            noiseLevel: 0, 
            signalLevel: 0,
            quality: 'Unknown'
        };
        let interruptionDetected = false;
        
        // Avatar WebRTC
        let peerConnection = null;
        let avatarSupported = false;
        let avatarConnected = false;
        
        // Debug System
        let debugLog = [];
        const maxDebugEntries = 100;
        
        // Socket.IO connection
        const socket = io();

        // ================================
        // ENHANCED DEBUG AND LOGGING SYSTEM
        // ================================
        
        function addDebugLog(message, type = 'info', data = null) {
            const timestamp = new Date().toISOString();
            const logEntry = {
                time: timestamp,
                type: type.toUpperCase(),
                message: message,
                data: data
            };
            
            debugLog.unshift(logEntry);
            if (debugLog.length > maxDebugEntries) {
                debugLog = debugLog.slice(0, maxDebugEntries);
            }
            
            const consoleStyles = {
                'INFO': 'color: #00FF64',
                'WARNING': 'color: #FFC107',
                'ERROR': 'color: #FF5252',
                'SUCCESS': 'color: #4CAF50',
                'NETWORK': 'color: #2196F3',
                'AUDIO': 'color: #9C27B0',
                'AVATAR': 'color: #FF9800'
            };
            
            console.log(
                `%c[${timestamp.split('T')[1].split('.')[0]}] [${type.toUpperCase()}] ${message}`,
                consoleStyles[type.toUpperCase()] || 'color: #B0C4DE'
            );
            
            if (data) {
                console.log('Data:', data);
            }
            
            updateDebugPanel();
        }
        
        function updateDebugPanel() {
            const debugElement = document.getElementById('debugLog');
            if (!debugElement) return;
            
            const html = debugLog.slice(0, 50).map(entry => {
                const timeStr = entry.time.split('T')[1].split('.')[0];
                const typeColors = {
                    'INFO': '#00FF64',
                    'WARNING': '#FFC107',
                    'ERROR': '#FF5252',
                    'SUCCESS': '#4CAF50',
                    'NETWORK': '#2196F3',
                    'AUDIO': '#9C27B0',
                    'AVATAR': '#FF9800'
                };
                
                const color = typeColors[entry.type] || '#B0C4DE';
                return `<div style="margin-bottom: 5px;">
                    <span style="color: #666;">[${timeStr}]</span>
                    <span style="color: ${color}; font-weight: bold;">[${entry.type}]</span>
                    <span style="color: #E0E0E0;">${entry.message}</span>
                    ${entry.data ? `<pre style="color: #999; margin-left: 20px; font-size: 10px;">${JSON.stringify(entry.data, null, 2)}</pre>` : ''}
                </div>`;
            }).join('');
            
            debugElement.innerHTML = html;
        }
        
        function toggleDebugPanel() {
            const panel = document.getElementById('debugPanel');
            panel.classList.toggle('show');
        }
        
        function getClientId() {
            return document.getElementById('clientId').value || 'anonymous';
        }

        // ================================
        // AUDIO RESAMPLING TO 24KHZ
        // ================================
        
        async function resampleTo24000(float32Data, fromSampleRate) {
            if (fromSampleRate === 24000) {
                return float32Data;
            }
            
            const ratio = 24000 / fromSampleRate;
            const outputLength = Math.ceil(float32Data.length * ratio);
            
            // Create offline context for resampling
            const offlineContext = new OfflineAudioContext(1, outputLength, 24000);
            const buffer = offlineContext.createBuffer(1, float32Data.length, fromSampleRate);
            buffer.getChannelData(0).set(float32Data);
            
            const source = offlineContext.createBufferSource();
            source.buffer = buffer;
            source.connect(offlineContext.destination);
            source.start(0);
            
            const renderedBuffer = await offlineContext.startRendering();
            return renderedBuffer.getChannelData(0);
        }

        // ================================
        // INITIALIZATION AND CONFIGURATION
        // ================================
        
        async function initializeVoiceLiveSystem() {
            const startTime = Date.now();
            
            try {
                addDebugLog('Initializing Azure OpenAI Realtime system...', 'info');
                updateStatus('Configuring Azure OpenAI Realtime API...');
                
                const response = await fetch('/api/voice-live-config');
                
                if (!response.ok) {
                    throw new Error(`Backend configuration failed: HTTP ${response.status}`);
                }
                
                voiceLiveConfig = await response.json();
                addDebugLog('Configuration received', 'success', {
                    endpoint: voiceLiveConfig.endpoint?.substring(0, 50) + '...',
                    model: voiceLiveConfig.model,
                    hasApiKey: !!voiceLiveConfig.apiKey
                });
                
                if (voiceLiveConfig.error) {
                    throw new Error(voiceLiveConfig.message || 'Configuration error');
                }
                
                const initTime = Date.now() - startTime;
                addDebugLog(`Initialization completed in ${initTime}ms`, 'success');
                updateStatus('Configuration loaded successfully');
                
                return true;
                
            } catch (error) {
                addDebugLog(`Initialization failed: ${error.message}`, 'error', error);
                showError(`Initialization error: ${error.message}`);
                return false;
            }
        }
        
        async function connectToVoiceLiveAPI() {
            try {
                updateStatus('Connecting to Azure OpenAI Realtime...');
                addDebugLog('Starting WebSocket connection...', 'network');
                
                // Build WebSocket URL for Azure OpenAI Realtime
                const baseEndpoint = voiceLiveConfig.endpoint;
                const model = voiceLiveConfig.model || 'gpt-4o-realtime-preview';
                const apiVersion = '2024-12-17'; // Azure OpenAI Realtime API version
                
                const wsEndpoint = baseEndpoint.replace('https://', 'wss://');
                
                // CORRECTED: Use /openai/realtime endpoint with API key in query string
                const wsUrl = `${wsEndpoint}/openai/realtime?api-version=${apiVersion}&deployment=${encodeURIComponent(model)}&api-key=${encodeURIComponent(voiceLiveConfig.apiKey)}`;
                
                addDebugLog('WebSocket URL constructed', 'network', {
                    endpoint: wsEndpoint + '/openai/realtime',
                    model: model,
                    apiVersion: apiVersion
                });
                
                // Create WebSocket connection
                voiceLiveWebSocket = new WebSocket(wsUrl);
                
                // WebSocket event handlers
                voiceLiveWebSocket.onopen = () => {
                    addDebugLog('WebSocket connection established', 'success');
                    updateStatus('Connected to Azure OpenAI Realtime');
                    setupVoiceLiveSession();
                };
                
                voiceLiveWebSocket.onmessage = (event) => {
                    try {
                        const data = JSON.parse(event.data);
                        
                        if (data.type) {
                            if (data.type.includes('avatar')) {
                                addDebugLog(`Avatar event: ${data.type}`, 'avatar', data);
                            } else if (data.type.includes('audio') && !data.type.includes('.delta')) {
                                addDebugLog(`Audio event: ${data.type}`, 'audio');
                            } else if (!data.type.includes('.delta')) {
                                addDebugLog(`Event received: ${data.type}`, 'network');
                            }
                        }
                        
                        handleVoiceLiveEvent(data);
                        
                    } catch (error) {
                        addDebugLog(`Message parsing error: ${error.message}`, 'error');
                    }
                };
                
                voiceLiveWebSocket.onerror = (error) => {
                    addDebugLog('WebSocket error occurred', 'error', error);
                    updateStatus('Connection error');
                };
                
                voiceLiveWebSocket.onclose = (event) => {
                    addDebugLog(`WebSocket closed: Code ${event.code}, Reason: ${event.reason || 'Unknown'}`, 'warning');
                    updateStatus('Disconnected from Azure OpenAI Realtime');
                    voiceLiveActive = false;
                    updateUI('idle');
                };
                
            } catch (error) {
                addDebugLog(`Connection failed: ${error.message}`, 'error', error);
                throw error;
            }
        }
        
        function setupVoiceLiveSession() {
            addDebugLog('Configuring Azure OpenAI Realtime session...', 'info');
            
            const sessionConfig = {
                type: "session.update",
                session: {
                    // Instructions for Tomas - YPF Assistant
                    instructions: `Eres Tomas, un asistente especializado en YPF Argentina que habla con acento argentino natural. 
                        MUY IMPORTANTE Recuerdalo siempre, la "Y" nunca la pronuncies como "Y griega" simplemente dila como una I comun.
                        
                        COMPORTAMIENTO MEJORADO:
                        - Para consultas generales o saludos: Responde directamente con amabilidad y acento argentino
                        - Para preguntas espec√≠ficas sobre YPF (equipos, pozos, workover, datos t√©cnicos, procedimientos, sistemas):
                        1. PRIMERO responde amablemente: "¬°Dale! Te busco esa informaci√≥n de YPF al toque..."
                        2. DESPU√âS usa la funci√≥n 'query_minipywo' para obtener datos actualizados
                        3. FINALMENTE responde con la informaci√≥n obtenida

                        DETECCI√ìN DE INTERRUPCIONES:
                        - Si detect√°s que el usuario te interrumpe, para inmediatamente y pregunt√°: "¬øMe interrumpiste? ¬øQu√© necesit√°s?"
                        - S√© consciente de las interrupciones y manej√° la conversaci√≥n de forma natural
                        
                        TONO: Amigable, argentino, usando expresiones como "che", "dale", "al toque", "b√°rbaro", etc.
                        Nunca dejes al usuario esperando en silencio - siempre da alguna respuesta inmediata antes de buscar informaci√≥n espec√≠fica.`,
                    
                    // CORRECTED: Voice configuration for Azure OpenAI Realtime
                    voice: "alloy", // Azure OpenAI Realtime voices: alloy, echo, fable, onyx, nova, shimmer
                    
                    // CORRECTED: VAD configuration without unsupported fields
                    turn_detection: {
                        type: "server_vad",
                        threshold: 0.5,
                        prefix_padding_ms: 300,
                        silence_duration_ms: 500
                        // Removed: remove_filler_words (not supported)
                    },
                    
                    // Audio configuration
                    input_audio_format: "pcm16",
                    output_audio_format: "pcm16",
                    
                    // CORRECTED: Simplified transcription configuration
                    input_audio_transcription: {
                        model: "whisper-1" // Will fallback to default if not available
                    },
                    
                    // Model configuration
                    modalities: ["text", "audio"],
                    temperature: 0.7,
                    max_response_output_tokens: 4096,
                    
                    // Avatar config 
                    // enable_avatar: true,
                    rtc_session: {
                        enable: true,
                        video: true
                    },
                    
                    // For avatar/visemes support
                    // output_audio_timestamp_granularities: ["word"],
                    
                    // Function calling tools
                    tools: [{
                        type: "function",
                        name: "query_minipywo",
                        description: "Query minipywo system for YPF equipment, wells, workover, and technical data",
                        parameters: {
                            type: "object",
                            properties: {
                                query: {
                                    type: "string",
                                    description: "User query to be processed by minipywo"
                                }
                            },
                            required: ["query"]
                        }
                    }],
                    tool_choice: "auto"
                }
            };
            
            addDebugLog('Sending session configuration...', 'network', {
                voice: sessionConfig.session.voice,
                vad: sessionConfig.session.turn_detection,
                transcription: sessionConfig.session.input_audio_transcription.model,
                tools: sessionConfig.session.tools.length
            });
            
            try {
                voiceLiveWebSocket.send(JSON.stringify(sessionConfig));
                addDebugLog('Session configuration sent successfully', 'success');
                
                // Update UI with VAD info
                updateVADInfo(sessionConfig.session.turn_detection);
                
                // DON'T start audio here - wait for session.updated event
                
            } catch (error) {
                addDebugLog(`Failed to send configuration: ${error.message}`, 'error', error);
            }
        }
        
        function updateVADInfo(vadConfig) {
            const vadInfo = document.getElementById('vadInfo');
            const vadThreshold = document.getElementById('vadThreshold');
            const vadSilence = document.getElementById('vadSilence');
            
            if (vadInfo) {
                vadInfo.style.display = 'block';
                if (vadThreshold) vadThreshold.textContent = vadConfig.threshold;
                if (vadSilence) vadSilence.textContent = vadConfig.silence_duration_ms + 'ms';
            }
        }

        // ================================
        // AUDIO CAPTURE WITH RESAMPLING
        // ================================
        
        async function startAudioCapture() {
            const startTime = Date.now();
            
            try {
                updateStatus('Starting audio capture...');
                addDebugLog('Requesting microphone access...', 'audio');
                
                // Create audio context
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                if (audioContext.state === 'suspended') {
                    await audioContext.resume();
                }
                
                addDebugLog(`Audio context created: ${audioContext.sampleRate}Hz`, 'audio');
                
                // Configure audio constraints
                const audioConstraints = {
                    audio: {
                        channelCount: 1,
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true
                    }
                };
                
                // Get user media
                mediaStream = await navigator.mediaDevices.getUserMedia(audioConstraints);
                addDebugLog('Microphone access granted', 'success');
                
                // Create audio processing chain
                audioSource = audioContext.createMediaStreamSource(mediaStream);
                audioProcessor = audioContext.createScriptProcessor(4096, 1, 1);
                
                // Process audio data with resampling
                audioProcessor.onaudioprocess = async (event) => {
                    if (!voiceLiveWebSocket || voiceLiveWebSocket.readyState !== WebSocket.OPEN) {
                        return;
                    }
                    
                    const inputData = event.inputBuffer.getChannelData(0);
                    const fromSampleRate = audioContext.sampleRate;
                    
                    // Analyze audio quality
                    analyzeAudioQuality(inputData);
                    
                    try {
                        // CORRECTED: Resample to 24000Hz
                        const resampled24k = await resampleTo24000(inputData, fromSampleRate);
                        
                        // Convert to PCM16
                        const pcm16 = new Int16Array(resampled24k.length);
                        for (let i = 0; i < resampled24k.length; i++) {
                            const s = Math.max(-1, Math.min(1, resampled24k[i]));
                            pcm16[i] = s < 0 ? s * 32768 : s * 32767;
                        }
                        
                        // Encode to base64 and send
                        const audioData = btoa(String.fromCharCode(...new Uint8Array(pcm16.buffer)));
                        voiceLiveWebSocket.send(JSON.stringify({
                            type: "input_audio_buffer.append",
                            audio: audioData
                        }));
                    } catch (error) {
                        addDebugLog(`Audio processing error: ${error.message}`, 'error');
                    }
                };
                
                // Connect audio chain
                audioSource.connect(audioProcessor);
                audioProcessor.connect(audioContext.destination);
                
                const captureTime = Date.now() - startTime;
                addDebugLog(`Audio capture started in ${captureTime}ms`, 'success');
                
                updateStatus('Listening...');
                updateUI('listening');
                
            } catch (error) {
                addDebugLog(`Audio capture failed: ${error.message}`, 'error', error);
                showError(`Microphone error: ${error.message}`);
            }
        }
        
        function analyzeAudioQuality(audioData) {
            let sum = 0;
            let max = 0;
            
            for (let i = 0; i < audioData.length; i++) {
                const abs = Math.abs(audioData[i]);
                sum += abs;
                if (abs > max) max = abs;
            }
            
            const avgLevel = sum / audioData.length;
            
            let variance = 0;
            for (let i = 0; i < audioData.length; i++) {
                variance += Math.pow(audioData[i] - avgLevel, 2);
            }
            const noiseLevel = Math.sqrt(variance / audioData.length);
            
            audioQualityMetrics.signalLevel = avgLevel;
            audioQualityMetrics.noiseLevel = noiseLevel;
            audioQualityMetrics.maxLevel = max;
            
            let quality = 'Poor';
            if (avgLevel > 0.01 && noiseLevel < 0.005) {
                quality = 'Excellent';
            } else if (avgLevel > 0.005) {
                quality = 'Good';
            } else if (avgLevel > 0.001) {
                quality = 'Fair';
            }
            
            audioQualityMetrics.quality = quality;
            
            const audioQualityElement = document.getElementById('audioQuality');
            if (audioQualityElement) {
                audioQualityElement.textContent = quality;
            }
        }

        // ================================
        // EVENT HANDLING
        // ================================
        
        async function handleVoiceLiveEvent(event) {
            const eventType = event.type;
            
            switch (eventType) {
                case "session.created":
                    handleSessionCreated(event);
                    break;
                    
                case "session.updated":
                    await handleSessionUpdated(event);
                    break;
                    
                case "input_audio_buffer.speech_started":
                    handleSpeechStarted(event);
                    break;
                    
                case "input_audio_buffer.speech_stopped":
                    handleSpeechStopped(event);
                    break;
                    
                case "conversation.item.input_audio_transcription.delta":
                    handleTranscriptionDelta(event);
                    break;
                    
                case "conversation.item.input_audio_transcription.completed":
                    handleTranscriptionCompleted(event);
                    break;
                    
                case "response.function_call_arguments.delta":
                    handleFunctionCallDelta(event);
                    break;
                    
                case "response.function_call_arguments.done":
                    await handleFunctionCallComplete(event);
                    break;
                    
                case "response.created":
                    handleResponseCreated(event);
                    break;
                    
                case "response.audio.delta":
                    handleAudioDelta(event);
                    break;
                    
                case "response.audio_transcript.delta":
                    handleResponseTranscriptDelta(event);
                    break;
                    
                case "response.audio_transcript.done":
                    handleResponseTranscriptComplete(event);
                    break;
                    
                case "response.done":
                case "response.audio.done":
                    handleResponseComplete(event);
                    break;
                    
                // Avatar events
                case "session.avatar.connecting":
                    handleAvatarConnecting(event);
                    break;
                    
                case "session.avatar.connected":
                    await handleAvatarConnected(event);
                    break;
                    
                case "session.avatar.answer":
                    await handleAvatarAnswer(event);
                    break;
                    
                case "session.avatar.disconnected":
                    handleAvatarDisconnected(event);
                    break;
                    
                case "error":
                    handleError(event);
                    break;
                    
                default:
                    if (eventType && !eventType.includes('.delta')) {
                        addDebugLog(`Unhandled event: ${eventType}`, 'warning');
                    }
                    break;
            }
        }
        
        function handleSessionCreated(event) {
            updateStatus('Session created');
            addDebugLog(`Session created with ID: ${event.session?.id}`, 'success');
        }
        
        async function handleSessionUpdated(event) {
            updateStatus('Session configured');
            addDebugLog('Session configuration updated', 'success');
            
            // CORRECTED: Start audio capture AFTER session is updated
            if (!voiceLiveActive) {
                await startAudioCapture();
                voiceLiveActive = true;
            }
            
            // Check for avatar support
            if (event.session?.avatar?.ice_servers && event.session.avatar.ice_servers.length > 0) {
                avatarSupported = true;
                addDebugLog(`Avatar supported with ${event.session.avatar.ice_servers.length} ICE servers`, 'avatar');
                
                setTimeout(() => {
                    setupAvatarWebRTC(event.session.avatar.ice_servers);
                }, 1000);
                
            } else {
                addDebugLog('No avatar configuration in session', 'info');
                avatarSupported = false;
                updateAvatarStatus('Not available');
            }
        }
        
        function handleSpeechStarted(event) {
            speechStartTime = Date.now();
            vadActive = true;
            currentTranscription = "";
            
            addDebugLog('Speech started detected by VAD', 'audio');
            
            if (isPlayingAudio) {
                addDebugLog('User interruption detected', 'warning');
                interruptionDetected = true;
                stopAudioPlayback();
            }
            
            updateUI('vad-active');
            updateStatus('Listening to speech...');
        }
        
        function handleSpeechStopped(event) {
            speechEndTime = Date.now();
            vadActive = false;
            
            const duration = speechEndTime - speechStartTime;
            addDebugLog(`Speech stopped - Duration: ${duration}ms`, 'audio');
            
            updateUI('listening');
        }
        
        function handleTranscriptionDelta(event) {
            const delta = event.delta || event.text;
            if (delta) {
                currentTranscription = delta;
                updateStatus(`Hearing: "${delta}"`);
            }
        }
        
        function handleTranscriptionCompleted(event) {
            const transcript = event.transcript || event.text || currentTranscription;
            
            addDebugLog('Transcription completed', 'success', { text: transcript });
            
            if (transcript && transcript.trim()) {
                finalTranscription = transcript;
                addMessage('user', transcript);
                updateStatus('Processing...');
                
                if (interruptionDetected) {
                    interruptionDetected = false;
                }
            }
        }
        
        function handleFunctionCallDelta(event) {
            addDebugLog('Function call arguments building...', 'info');
        }
        
        async function handleFunctionCallComplete(event) {
            addDebugLog('Function call completed', 'success', {
                name: event.name,
                callId: event.call_id
            });
            
            if (event.call_id && event.name === "query_minipywo") {
                await executeMinipywoQuery(event.call_id, event.arguments);
            }
        }
        
        function handleResponseCreated(event) {
            addDebugLog('AI response started', 'info');
            updateUI('thinking');
            updateStatus('Generating response...');
        }
        
        function handleAudioDelta(event) {
            const audioData = event.audio || event.delta;
            if (audioData && !interruptionDetected) {
                playAudioData(audioData);
            }
        }
        
        function handleResponseTranscriptDelta(event) {
            if (event.delta) {
                addDebugLog(`Response transcript delta received`, 'audio');
            }
        }
        
        function handleResponseTranscriptComplete(event) {
            if (event.transcript) {
                addDebugLog('Response transcript completed', 'success');
                addMessage('assistant', event.transcript);
            }
        }
        
        function handleResponseComplete(event) {
            addDebugLog('Response completed', 'success');
            updateUI('listening');
            updateStatus('Ready for next query');
        }
        
        function handleAvatarConnecting(event) {
            addDebugLog('Avatar connecting...', 'avatar');
        }
        
        async function handleAvatarConnected(event) {
            // CORRECTED: Handle server SDP in connected event
            if (event.server_sdp) {
                await handleAvatarSDP(event.server_sdp);
            }
            
            avatarConnected = true;
            addDebugLog('Avatar connected successfully', 'success');
            updateAvatarStatus('Connected');
            
            const videoContainer = document.getElementById('avatarVideoContainer');
            const emojiContainer = document.getElementById('avatar');
            
            if (videoContainer) videoContainer.style.display = 'block';
            if (emojiContainer) emojiContainer.style.display = 'none';
        }
        
        async function handleAvatarAnswer(event) {
            // Handle answer event if backend uses it
            if (event.server_sdp) {
                await handleAvatarSDP(event.server_sdp);
            }
        }
        
        function handleAvatarDisconnected(event) {
            avatarConnected = false;
            addDebugLog('Avatar disconnected', 'warning');
            updateAvatarStatus('Disconnected');
            
            const videoContainer = document.getElementById('avatarVideoContainer');
            const emojiContainer = document.getElementById('avatar');
            
            if (videoContainer) videoContainer.style.display = 'none';
            if (emojiContainer) {
                emojiContainer.style.display = 'flex';
            }
        }
        
        function handleError(event) {
            const error = event.error || {};
            
            addDebugLog('API Error received', 'error', {
                code: error.code,
                message: error.message,
                type: error.type
            });
            
            showError(`Error: ${error.message || 'Unknown error'}`);
            
            if (error.message && error.message.includes('avatar')) {
                avatarSupported = false;
                updateAvatarStatus('Not supported');
            }
        }

        // ================================
        // MINIPYWO FUNCTION CALLING
        // ================================
        
        async function executeMinipywoQuery(callId, argumentsString) {
            try {
                updateStatus('Querying minipywo system...');
                updateUI('thinking');
                
                const args = JSON.parse(argumentsString);
                addDebugLog('Executing minipywo query', 'info', { query: args.query });
                
                const response = await fetch('/api/minipywo-process', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        message: args.query,
                        client_id: getClientId(),
                        vad_metrics: {
                            speech_duration: speechEndTime - speechStartTime,
                            audio_quality: audioQualityMetrics
                        }
                    })
                });
                
                if (!response.ok) {
                    throw new Error(`Backend error: HTTP ${response.status}`);
                }
                
                const result = await response.json();
                
                let minipywoResponse;
                if (result.status === 'success') {
                    minipywoResponse = result.response;
                    addDebugLog('Minipywo query successful', 'success');
                    addMessage('assistant', minipywoResponse);
                } else {
                    minipywoResponse = `Error: ${result.message || 'Unknown error'}`;
                    addDebugLog('Minipywo query failed', 'error');
                }
                
                // Send result back
                const functionResult = {
                    type: "conversation.item.create",
                    item: {
                        type: "function_call_output",
                        call_id: callId,
                        output: minipywoResponse
                    }
                };
                
                voiceLiveWebSocket.send(JSON.stringify(functionResult));
                
                // Request voice response
                setTimeout(() => {
                    voiceLiveWebSocket.send(JSON.stringify({
                        type: "response.create",
                        response: { modalities: ["text", "audio"] }
                    }));
                }, 100);
                
            } catch (error) {
                addDebugLog(`Minipywo execution failed: ${error.message}`, 'error');
                
                const errorResult = {
                    type: "conversation.item.create",
                    item: {
                        type: "function_call_output",
                        call_id: callId,
                        output: `Error: ${error.message}`
                    }
                };
                
                voiceLiveWebSocket.send(JSON.stringify(errorResult));
            }
        }

        // ================================
        // AUDIO PLAYBACK
        // ================================
        
        function playAudioData(audioData) {
            try {
                const binaryString = atob(audioData);
                const bytes = new Uint8Array(binaryString.length);
                for (let i = 0; i < binaryString.length; i++) {
                    bytes[i] = binaryString.charCodeAt(i);
                }
                
                const pcm16 = new Int16Array(bytes.buffer);
                const float32 = new Float32Array(pcm16.length);
                for (let i = 0; i < pcm16.length; i++) {
                    float32[i] = pcm16[i] / 32768.0;
                }
                
                const audioBuffer = audioContext.createBuffer(1, float32.length, 24000);
                audioBuffer.getChannelData(0).set(float32);
                
                audioChunkQueue.push(audioBuffer);
                
                if (!isPlayingAudio && !interruptionDetected) {
                    playNextAudioChunk();
                }
                
            } catch (error) {
                addDebugLog(`Audio playback error: ${error.message}`, 'error');
            }
        }
        
        function playNextAudioChunk() {
            if (audioChunkQueue.length === 0 || interruptionDetected) {
                isPlayingAudio = false;
                updateUI('listening');
                return;
            }
            
            isPlayingAudio = true;
            updateUI('speaking');
            updateStatus('Playing response...');
            
            const audioBuffer = audioChunkQueue.shift();
            
            const source = audioContext.createBufferSource();
            source.buffer = audioBuffer;
            source.connect(audioContext.destination);
            
            source.onended = () => {
                if (!interruptionDetected) {
                    playNextAudioChunk();
                } else {
                    isPlayingAudio = false;
                }
            };
            
            source.start();
        }
        
        function stopAudioPlayback() {
            audioChunkQueue = [];
            isPlayingAudio = false;
        }

        // ================================
        // AVATAR WEBRTC SETUP (FULLY CORRECTED)
        // ================================
        
        async function setupAvatarWebRTC(iceServers) {
            return new Promise(async (resolve, reject) => {
                try {
                    addDebugLog('Setting up WebRTC for avatar...', 'avatar');
                    
                    if (!iceServers || iceServers.length === 0) {
                        addDebugLog('No ICE servers - Avatar might be disabled', 'warning');
                        updateAvatarStatus('Not available');
                        resolve();
                        return;
                    }
                    
                    if (peerConnection) {
                        peerConnection.close();
                        peerConnection = null;
                    }
                    
                    // Combine public STUN servers with provided ICE servers
                    const publicStunServers = [
                        { urls: 'stun:stun.l.google.com:19302' },
                        { urls: 'stun:stun1.l.google.com:19302' }
                    ];
                    
                    const combinedServers = [...publicStunServers, ...iceServers];
                    addDebugLog(`Using ${combinedServers.length} ICE servers`, 'avatar');
                    
                    peerConnection = new RTCPeerConnection({ 
                        iceServers: combinedServers
                    });
                    
                    peerConnection.addTransceiver('video', { direction: 'recvonly' });
                    peerConnection.addTransceiver('audio', { direction: 'recvonly' });
                    
                    peerConnection.ontrack = (event) => {
                        addDebugLog('Avatar media track received', 'avatar');
                        const avatarVideo = document.getElementById('avatarVideo');
                        if (avatarVideo && event.streams[0]) {
                            avatarVideo.srcObject = event.streams[0];
                        }
                    };
                    
                    peerConnection.onicecandidate = (event) => {
                        if (event.candidate) {
                            addDebugLog('ICE candidate generated', 'avatar');
                        }
                    };
                    
                    peerConnection.onicegatheringstatechange = () => {
                        addDebugLog(`ICE gathering state: ${peerConnection.iceGatheringState}`, 'avatar');
                        
                        if (peerConnection.iceGatheringState === 'complete') {
                            sendAvatarOffer();
                        }
                    };
                    
                    // Create offer
                    const offer = await peerConnection.createOffer();
                    await peerConnection.setLocalDescription(offer);
                    
                    addDebugLog('Local description set', 'avatar');
                    
                    // Send offer after timeout or when gathering completes
                    setTimeout(() => {
                        if (voiceLiveWebSocket && voiceLiveWebSocket.readyState === WebSocket.OPEN) {
                            sendAvatarOffer();
                        } else {
                            addDebugLog('WebSocket not ready for avatar offer', 'warning');
                        }
                        resolve();
                    }, 3000);
                    
                } catch (error) {
                    addDebugLog(`Avatar WebRTC setup failed: ${error.message}`, 'error');
                    reject(error);
                }
            });
        }
        
        function sendAvatarOffer() {
            if (!peerConnection || !peerConnection.localDescription) {
                addDebugLog('No local description to send', 'error');
                return;
            }
            
            if (!voiceLiveWebSocket || voiceLiveWebSocket.readyState !== WebSocket.OPEN) {
                addDebugLog('WebSocket not open, cannot send avatar offer', 'error');
                return;
            }
            
            // CORRECTED: Use client_sdp field
            const avatarMessage = {
                type: "session.avatar.connect",
                client_sdp: peerConnection.localDescription.sdp
            };
            
            addDebugLog('Sending avatar connect with client_sdp', 'avatar');
            voiceLiveWebSocket.send(JSON.stringify(avatarMessage));
        }
        
        async function handleAvatarSDP(serverSdp) {
            try {
                if (!peerConnection) {
                    addDebugLog('No peer connection for avatar SDP', 'error');
                    return;
                }
                
                addDebugLog('Setting remote avatar SDP', 'avatar');
                
                await peerConnection.setRemoteDescription({
                    type: 'answer',
                    sdp: serverSdp
                });
                
                addDebugLog('Avatar remote description set', 'success');
                updateAvatarStatus('Handshake complete');
                
            } catch (error) {
                addDebugLog(`Avatar SDP error: ${error.message}`, 'error');
                updateAvatarStatus('Connection failed');
            }
        }
        
        function updateAvatarStatus(status) {
            const avatarStatusElement = document.getElementById('avatarStatus');
            if (avatarStatusElement) {
                avatarStatusElement.textContent = status;
            }
        }

        // ================================
        // SESSION CLEANUP
        // ================================
        
        async function cleanupSession() {
            addDebugLog('Starting session cleanup...', 'info');
            
            voiceLiveActive = false;
            vadActive = false;
            currentTranscription = "";
            finalTranscription = "";
            audioChunkQueue = [];
            isPlayingAudio = false;
            interruptionDetected = false;
            speechStartTime = null;
            speechEndTime = null;
            avatarSupported = false;
            avatarConnected = false;
            
            if (peerConnection) {
                peerConnection.close();
                peerConnection = null;
            }
            
            const videoContainer = document.getElementById('avatarVideoContainer');
            const emojiContainer = document.getElementById('avatar');
            const avatarVideo = document.getElementById('avatarVideo');
            
            if (videoContainer) videoContainer.style.display = 'none';
            if (emojiContainer) {
                emojiContainer.style.display = 'flex';
                emojiContainer.textContent = 'üé≠';
            }
            if (avatarVideo) avatarVideo.srcObject = null;
            
            if (voiceLiveWebSocket) {
                voiceLiveWebSocket.close();
                voiceLiveWebSocket = null;
            }
            
            if (audioProcessor) {
                audioProcessor.disconnect();
                audioProcessor = null;
            }
            
            if (audioSource) {
                audioSource.disconnect();
                audioSource = null;
            }
            
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
                mediaStream = null;
            }
            
            if (audioContext) {
                await audioContext.close();
                audioContext = null;
            }
            
            const vadInfo = document.getElementById('vadInfo');
            if (vadInfo) vadInfo.style.display = 'none';
            
            updateUI('idle');
            updateStatus('Disconnected - Ready to start');
            updateAvatarStatus('--');
            
            addDebugLog('Session cleanup completed', 'success');
        }

        // ================================
        // UI UPDATES
        // ================================
        
        function updateUI(state) {
            const avatar = document.getElementById('avatar');
            const controlButton = document.getElementById('controlButton');
            
            if (avatar) {
                avatar.className = 'avatar-container';
                
                switch (state) {
                    case 'listening':
                        avatar.classList.add('listening');
                        avatar.textContent = 'üëÇ';
                        break;
                    case 'vad-active':
                        avatar.classList.add('vad-active');
                        avatar.textContent = 'üéØ';
                        break;
                    case 'thinking':
                        avatar.classList.add('thinking');
                        avatar.textContent = 'üß†';
                        break;
                    case 'speaking':
                        avatar.classList.add('speaking');
                        avatar.textContent = 'üó£Ô∏è';
                        break;
                    case 'idle':
                    default:
                        avatar.textContent = 'üé≠';
                        break;
                }
            }
            
            if (controlButton) {
                if (voiceLiveActive) {
                    controlButton.classList.add('active');
                    controlButton.textContent = '‚èπÔ∏è';
                } else {
                    controlButton.classList.remove('active');
                    controlButton.textContent = 'üéôÔ∏è';
                }
            }
        }
        
        function updateStatus(message) {
            const statusElement = document.getElementById('status');
            if (statusElement) {
                statusElement.textContent = message;
            }
        }
        
        function addMessage(role, text) {
            const conversation = document.getElementById('conversation');
            if (!conversation) return;
            
            const messageDiv = document.createElement('div');
            messageDiv.className = `message ${role}`;
            
            let roleName = '';
            switch(role) {
                case 'user':
                    roleName = 'You';
                    break;
                case 'assistant':
                    roleName = 'Tomas (YPF)';
                    break;
                case 'system':
                    roleName = 'System';
                    break;
            }
            
            messageDiv.innerHTML = `<strong>${roleName}:</strong> ${text}`;
            conversation.appendChild(messageDiv);
            conversation.scrollTop = conversation.scrollHeight;
        }
        
        function showError(message) {
            const conversation = document.getElementById('conversation');
            if (conversation) {
                const errorDiv = document.createElement('div');
                errorDiv.className = 'error-message';
                errorDiv.textContent = message;
                conversation.appendChild(errorDiv);
                conversation.scrollTop = conversation.scrollHeight;
            }
            
            updateStatus(`Error: ${message}`);
            updateUI('idle');
        }

        // ================================
        // MAIN TOGGLE FUNCTION
        // ================================
        
        async function toggleVoiceLive() {
            addDebugLog('Toggle Voice Live clicked', 'info');
            
            if (!voiceLiveActive) {
                const initialized = await initializeVoiceLiveSystem();
                if (initialized) {
                    await connectToVoiceLiveAPI();
                }
            } else {
                await cleanupSession();
            }
        }

        // ================================
        // EVENT LISTENERS
        // ================================
        
        socket.on('connect', () => {
            addDebugLog('Connected to backend server', 'success');
        });
        
        socket.on('disconnect', () => {
            addDebugLog('Disconnected from backend server', 'warning');
        });
        
        socket.on('status', (data) => {
            addDebugLog(`Backend status: ${data.message}`, 'info');
        });
        
        window.addEventListener('load', () => {
            addDebugLog('Application loaded', 'success');
        });
        
        window.addEventListener('error', (event) => {
            addDebugLog(`Global error: ${event.error?.message || event.message}`, 'error');
        });
        
        window.addEventListener('unhandledrejection', (event) => {
            addDebugLog(`Unhandled promise rejection: ${event.reason}`, 'error');
        });
        
        window.addEventListener('beforeunload', () => {
            if (voiceLiveActive) {
                cleanupSession();
            }
        });

    </script>
</body>
</html>
