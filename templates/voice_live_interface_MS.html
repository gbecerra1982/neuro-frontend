<!DOCTYPE html>
<html lang="es-AR">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>YPF Voice Live + Sistema minipywo - PRODUCTION</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #0a1428 0%, #1a2744 50%, #2a3754 100%);
            color: white;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            padding: 20px;
        }

        .container {
            text-align: center;
            max-width: 1000px;
            width: 100%;
        }

        .header {
            margin-bottom: 40px;
        }

        .ypf-logo {
            background: linear-gradient(135deg, #007AFF 0%, #0056CC 100%);
            color: white;
            padding: 16px 32px;
            border-radius: 16px;
            font-weight: 800;
            font-size: 32px;
            letter-spacing: 3px;
            margin-bottom: 20px;
            display: inline-block;
            box-shadow: 0 8px 32px rgba(0, 122, 255, 0.4);
        }

        .title {
            font-size: 28px;
            margin-bottom: 12px;
            font-weight: 600;
        }

        .subtitle {
            color: #FF6B35;
            font-weight: 600;
            font-size: 18px;
            margin-bottom: 10px;
        }

        .improvements {
            background: rgba(0, 255, 100, 0.1);
            border: 2px solid rgba(0, 255, 100, 0.3);
            border-radius: 12px;
            padding: 15px;
            margin-bottom: 15px;
            text-align: left;
        }

        .improvements h3 {
            color: #00FF64;
            margin-bottom: 10px;
            font-size: 16px;
        }

        .improvements ul {
            font-size: 14px;
            color: #B0C4DE;
            line-height: 1.4;
        }

        .improvements li {
            margin-bottom: 5px;
        }

        .description {
            color: #B0C4DE;
            font-size: 16px;
            max-width: 700px;
            margin: 0 auto;
        }

        .main-interface {
            display: flex;
            gap: 40px;
            align-items: flex-start;
            justify-content: center;
            flex-wrap: wrap;
            margin: 40px 0;
        }

        .avatar-section {
            display: flex;
            flex-direction: column;
            align-items: center;
        }

        .avatar-container {
            width: 320px;
            height: 320px;
            border-radius: 24px;
            background: linear-gradient(135deg, #1a2744 0%, #2a3754 100%);
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 100px;
            margin-bottom: 30px;
            box-shadow: 0 0 40px rgba(0, 122, 255, 0.3);
            transition: all 0.3s ease;
            border: 3px solid rgba(255, 255, 255, 0.1);
        }

        .avatar-container.listening {
            animation: pulse-listening 1.5s infinite;
            box-shadow: 0 0 60px rgba(0, 255, 100, 0.6);
            border-color: rgba(0, 255, 100, 0.5);
        }

        .avatar-container.thinking {
            animation: pulse-thinking 0.8s infinite;
            box-shadow: 0 0 60px rgba(255, 193, 7, 0.6);
            border-color: rgba(255, 193, 7, 0.5);
        }

        .avatar-container.speaking {
            animation: pulse-speaking 1s infinite;
            box-shadow: 0 0 60px rgba(255, 107, 53, 0.8);
            border-color: rgba(255, 107, 53, 0.5);
        }

        .avatar-container.vad-active {
            animation: pulse-vad 0.6s infinite;
            box-shadow: 0 0 80px rgba(255, 20, 147, 0.8);
            border-color: rgba(255, 20, 147, 0.6);
        }

        @keyframes pulse-listening {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.02); }
        }

        @keyframes pulse-thinking {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.03); }
        }

        @keyframes pulse-speaking {
            0%, 100% { transform: scale(1); }
            25% { transform: scale(1.05); }
            75% { transform: scale(1.02); }
        }

        @keyframes pulse-vad {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.06); }
        }

        .avatar-video-container {
            width: 320px; 
            height: 320px; 
            border-radius: 24px; 
            overflow: hidden; 
            margin-bottom: 20px; 
            display: none; 
            box-shadow: 0 0 40px rgba(255, 107, 53, 0.4);
        }

        .control-section {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 20px;
        }

        .control-button {
            width: 120px;
            height: 120px;
            border-radius: 50%;
            border: none;
            cursor: pointer;
            font-size: 48px;
            background: linear-gradient(135deg, #00FF64 0%, #00CC51 100%);
            color: white;
            transition: all 0.15s ease;
            box-shadow: 0 12px 40px rgba(0, 0, 0, 0.3);
            position: relative;
            overflow: hidden;
        }

        .control-button:hover {
            transform: scale(1.05);
            box-shadow: 0 16px 48px rgba(0, 0, 0, 0.4);
        }

        .control-button:active {
            transform: scale(0.95);
        }

        .control-button.active {
            background: linear-gradient(135deg, #FF3B30 0%, #CC2E24 100%);
            animation: button-active 1s infinite;
        }

        @keyframes button-active {
            0%, 100% { box-shadow: 0 12px 40px rgba(255, 59, 48, 0.4); }
            50% { box-shadow: 0 16px 48px rgba(255, 59, 48, 0.6); }
        }

        .status {
            margin: 20px 0;
            font-weight: 600;
            color: #00FF64;
            font-size: 20px;
            min-height: 30px;
        }

        .vad-info {
            background: rgba(255, 20, 147, 0.1);
            border: 1px solid rgba(255, 20, 147, 0.3);
            border-radius: 10px;
            padding: 12px;
            margin: 15px 0;
            font-size: 14px;
            color: #FFB6C1;
        }

        .conversation-section {
            max-width: 900px;
            margin: 30px auto;
        }

        .conversation-header {
            font-size: 20px;
            font-weight: 600;
            margin-bottom: 15px;
            color: #B0C4DE;
        }

        .conversation {
            max-height: 400px;
            overflow-y: auto;
            text-align: left;
            background: rgba(255, 255, 255, 0.05);
            border-radius: 20px;
            padding: 20px;
            backdrop-filter: blur(10px);
            border: 1px solid rgba(255, 255, 255, 0.1);
        }

        .message {
            margin: 15px 0;
            padding: 15px 20px;
            border-radius: 20px;
            max-width: 85%;
            word-wrap: break-word;
        }

        .message.user {
            background: linear-gradient(135deg, rgba(0, 122, 255, 0.3) 0%, rgba(0, 122, 255, 0.1) 100%);
            margin-left: auto;
            margin-right: 0;
            border-bottom-right-radius: 5px;
        }

        .message.assistant {
            background: linear-gradient(135deg, rgba(255, 107, 53, 0.3) 0%, rgba(255, 107, 53, 0.1) 100%);
            margin-right: auto;
            margin-left: 0;
            border-bottom-left-radius: 5px;
        }

        .message.system {
            background: linear-gradient(135deg, rgba(255, 193, 7, 0.3) 0%, rgba(255, 193, 7, 0.1) 100%);
            margin: 0 auto;
            border-radius: 15px;
            font-size: 14px;
            max-width: 90%;
        }

        .message strong {
            color: #FFD700;
            display: block;
            margin-bottom: 5px;
            font-size: 14px;
            font-weight: 700;
        }

        .tech-info {
            margin-top: 40px;
            padding: 25px;
            background: rgba(255, 255, 255, 0.05);
            border-radius: 15px;
            font-size: 14px;
            color: #B0C4DE;
            backdrop-filter: blur(10px);
        }

        .tech-badges {
            display: flex;
            gap: 10px;
            justify-content: center;
            flex-wrap: wrap;
            margin-top: 15px;
        }

        .tech-badge {
            background: rgba(0, 122, 255, 0.2);
            color: #87CEEB;
            padding: 5px 12px;
            border-radius: 20px;
            font-size: 12px;
            font-weight: 600;
        }

        .tech-badge.new {
            background: rgba(0, 255, 100, 0.2);
            color: #90EE90;
        }

        .error-message {
            color: #FF6B6B;
            background: rgba(255, 107, 107, 0.1);
            padding: 15px;
            border-radius: 10px;
            margin: 20px 0;
            border-left: 4px solid #FF6B6B;
        }

        .debug-panel {
            position: fixed;
            top: 20px;
            right: 20px;
            background: rgba(0, 0, 0, 0.9);
            padding: 15px;
            border-radius: 10px;
            font-size: 12px;
            color: #90EE90;
            max-width: 400px;
            max-height: 500px;
            overflow-y: auto;
            display: none;
            border: 1px solid rgba(0, 255, 100, 0.3);
        }

        .debug-panel.show {
            display: block;
        }

        .debug-panel h4 {
            color: #00FF64;
            margin-bottom: 10px;
            border-bottom: 1px solid rgba(0, 255, 100, 0.2);
            padding-bottom: 5px;
        }

        #clientId {
            display: none;
        }

        @media (max-width: 768px) {
            .main-interface {
                flex-direction: column;
                gap: 20px;
            }

            .avatar-container {
                width: 280px;
                height: 280px;
                font-size: 80px;
            }

            .control-button {
                width: 100px;
                height: 100px;
                font-size: 40px;
            }

            .ypf-logo {
                font-size: 24px;
                padding: 12px 24px;
            }

            .title {
                font-size: 24px;
            }

            .debug-panel {
                position: relative;
                top: auto;
                right: auto;
                margin: 20px 0;
            }
        }
    </style>
</head>
<body>
    <input type="hidden" id="clientId" value="{{ client_id }}">

    <div class="container">
        <div class="header">
            <div class="ypf-logo">YPF üá¶üá∑</div>
            <div class="title">Voice Live + Sistema minipywo - PRODUCTION</div>
            <div class="subtitle">Azure Semantic VAD + Function Calling + Avatar Support</div>
            
            <div class="improvements">
                <h3>FEATURES IMPLEMENTED:</h3>
                <ul>
                    <li><strong>Azure Semantic VAD:</strong> Intelligent end-of-speech detection</li>
                    <li><strong>Whisper-1 Transcription:</strong> Accurate Spanish transcription with context</li>
                    <li><strong>Function Calling:</strong> Integration with minipywo system</li>
                    <li><strong>Avatar Support:</strong> Lisa avatar with WebRTC (when available)</li>
                    <li><strong>Interruption Detection:</strong> Smart handling of user interruptions</li>
                    <li><strong>Enhanced Logging:</strong> Comprehensive debug system</li>
                </ul>
            </div>
            
            <div class="description">
                Production-ready implementation with Microsoft Voice Live API for YPF operations.
                Optimized for Argentinian Spanish with technical petroleum terminology.
            </div>
        </div>

        <div class="main-interface">
            <div class="avatar-section">
                <div class="avatar-container" id="avatar">üé≠</div>
                <div class="avatar-video-container" id="avatarVideoContainer">
                    <video id="avatarVideo" autoplay muted style="width: 100%; height: 100%; object-fit: cover;"></video>
                </div>
                <div class="status" id="status">System ready - Click to start</div>
                <div class="vad-info" id="vadInfo" style="display: none;">
                    VAD: <span id="vadThreshold">0.5</span> | 
                    Silence: <span id="vadSilence">500ms</span> | 
                    Signal: <span id="audioQuality">--</span> |
                    Avatar: <span id="avatarStatus">--</span>
                </div>
            </div>

            <div class="control-section">
                <button class="control-button" id="controlButton" onclick="toggleVoiceLive()">üéôÔ∏è</button>
                <div style="color: #B0C4DE; font-size: 14px; max-width: 220px; text-align: center;">
                    Press to talk with Tomas - YPF Assistant
                </div>
                <button onclick="toggleDebugPanel()" style="padding: 8px 16px; background: rgba(255,255,255,0.1); border: none; color: white; border-radius: 8px; cursor: pointer; font-size: 12px;">
                    Debug Console
                </button>
            </div>
        </div>

        <div class="conversation-section">
            <div class="conversation-header">Conversation Log</div>
            <div class="conversation" id="conversation">
                <div class="message system">
                    <strong>System:</strong>
                    Voice Live API ready with Azure Semantic VAD, Whisper-1 transcription, 
                    and minipywo integration. Avatar support enabled when available.
                </div>
                <div class="message assistant">
                    <strong>Tomas (YPF):</strong>
                    ¬°Che! Soy Tomas, tu asistente de YPF. Puedo ayudarte con informaci√≥n sobre equipos, 
                    pozos, workover y cualquier dato t√©cnico que necesites. ¬°Dale, preguntame lo que quieras!
                </div>
            </div>
        </div>

        <div class="tech-info">
            <strong>Technical Implementation:</strong><br>
            Microsoft Voice Live API with server-side VAD, Whisper-1 transcription model,
            real-time function calling, and optional avatar support via WebRTC.
            
            <div class="tech-badges">
                <span class="tech-badge new">Azure Semantic VAD</span>
                <span class="tech-badge new">Whisper-1 STT</span>
                <span class="tech-badge">es-AR-TomasNeural</span>
                <span class="tech-badge">Function Calling</span>
                <span class="tech-badge">GPT-4o Realtime</span>
                <span class="tech-badge new">WebRTC Avatar</span>
                <span class="tech-badge">minipywo API</span>
            </div>
        </div>
    </div>

    <!-- Enhanced Debug Panel -->
    <div class="debug-panel" id="debugPanel">
        <h4>Debug Console - Voice Live API</h4>
        <div id="debugLog"></div>
    </div>

    <script src="https://cdn.socket.io/4.5.4/socket.io.min.js"></script>
    
    <script>
        // ================================
        // GLOBAL VARIABLES AND CONFIGURATION
        // ================================
        
        // Core WebSocket and Audio
        let voiceLiveWebSocket = null;
        let voiceLiveActive = false;
        let audioContext = null;
        let mediaStream = null;
        let audioProcessor = null;
        let audioSource = null;
        
        // Configuration
        let clientId = document.getElementById('clientId').value;
        let voiceLiveConfig = null;
        
        // Voice Activity Detection
        let vadActive = false;
        let speechStartTime = null;
        let speechEndTime = null;
        let currentTranscription = "";
        let finalTranscription = "";
        
        // Audio Processing
        let audioChunkQueue = [];
        let isPlayingAudio = false;
        let audioQualityMetrics = { 
            noiseLevel: 0, 
            signalLevel: 0,
            quality: 'Unknown'
        };
        let interruptionDetected = false;
        
        // Avatar WebRTC
        let peerConnection = null;
        let avatarSupported = false;
        let avatarConnected = false;
        
        // Debug System
        let debugLog = [];
        const maxDebugEntries = 100;
        let performanceMetrics = {
            wsLatency: 0,
            audioLatency: 0,
            transcriptionTime: 0
        };
        
        // Socket.IO connection
        const socket = io();

        // ================================
        // ENHANCED DEBUG AND LOGGING SYSTEM
        // ================================
        
        function addDebugLog(message, type = 'info', data = null) {
            const timestamp = new Date().toISOString();
            const logEntry = {
                time: timestamp,
                type: type.toUpperCase(),
                message: message,
                data: data
            };
            
            // Add to array
            debugLog.unshift(logEntry);
            if (debugLog.length > maxDebugEntries) {
                debugLog = debugLog.slice(0, maxDebugEntries);
            }
            
            // Console output with styling
            const consoleStyles = {
                'INFO': 'color: #00FF64',
                'WARNING': 'color: #FFC107',
                'ERROR': 'color: #FF5252',
                'SUCCESS': 'color: #4CAF50',
                'NETWORK': 'color: #2196F3',
                'AUDIO': 'color: #9C27B0',
                'AVATAR': 'color: #FF9800'
            };
            
            console.log(
                `%c[${timestamp.split('T')[1].split('.')[0]}] [${type.toUpperCase()}] ${message}`,
                consoleStyles[type.toUpperCase()] || 'color: #B0C4DE'
            );
            
            if (data) {
                console.log('Data:', data);
            }
            
            updateDebugPanel();
        }
        
        function updateDebugPanel() {
            const debugElement = document.getElementById('debugLog');
            if (!debugElement) return;
            
            const html = debugLog.slice(0, 50).map(entry => {
                const timeStr = entry.time.split('T')[1].split('.')[0];
                const typeColors = {
                    'INFO': '#00FF64',
                    'WARNING': '#FFC107',
                    'ERROR': '#FF5252',
                    'SUCCESS': '#4CAF50',
                    'NETWORK': '#2196F3',
                    'AUDIO': '#9C27B0',
                    'AVATAR': '#FF9800'
                };
                
                const color = typeColors[entry.type] || '#B0C4DE';
                return `<div style="margin-bottom: 5px;">
                    <span style="color: #666;">[${timeStr}]</span>
                    <span style="color: ${color}; font-weight: bold;">[${entry.type}]</span>
                    <span style="color: #E0E0E0;">${entry.message}</span>
                    ${entry.data ? `<pre style="color: #999; margin-left: 20px; font-size: 10px;">${JSON.stringify(entry.data, null, 2)}</pre>` : ''}
                </div>`;
            }).join('');
            
            debugElement.innerHTML = html;
        }
        
        function toggleDebugPanel() {
            const panel = document.getElementById('debugPanel');
            panel.classList.toggle('show');
            addDebugLog('Debug panel toggled', 'info');
        }
        
        function getClientId() {
            return document.getElementById('clientId').value || 'anonymous';
        }

        // ================================
        // INITIALIZATION AND CONFIGURATION
        // ================================
        
        async function initializeVoiceLiveSystem() {
            const startTime = Date.now();
            
            try {
                addDebugLog('Initializing Voice Live system...', 'info');
                updateStatus('Configuring Voice Live API...');
                
                // Fetch configuration from backend
                addDebugLog('Fetching configuration from backend...', 'network');
                const response = await fetch('/api/voice-live-config');
                
                if (!response.ok) {
                    throw new Error(`Backend configuration failed: HTTP ${response.status}`);
                }
                
                voiceLiveConfig = await response.json();
                addDebugLog('Configuration received', 'success', {
                    endpoint: voiceLiveConfig.endpoint?.substring(0, 30) + '...',
                    model: voiceLiveConfig.model,
                    hasApiKey: !!voiceLiveConfig.apiKey
                });
                
                if (voiceLiveConfig.error) {
                    throw new Error(voiceLiveConfig.message || 'Configuration error');
                }
                
                const initTime = Date.now() - startTime;
                addDebugLog(`Initialization completed in ${initTime}ms`, 'success');
                updateStatus('Configuration loaded successfully');
                
                return true;
                
            } catch (error) {
                addDebugLog(`Initialization failed: ${error.message}`, 'error', error);
                showError(`Initialization error: ${error.message}`);
                return false;
            }
        }
        
        async function connectToVoiceLiveAPI() {
            try {
                updateStatus('Connecting to Voice Live API...');
                addDebugLog('Starting WebSocket connection...', 'network');
                
                // Build WebSocket URL
                const baseEndpoint = voiceLiveConfig.endpoint;
                const model = voiceLiveConfig.model || 'gpt-4o';
                const apiVersion = '2025-05-01-preview';
                
                const wsEndpoint = baseEndpoint.replace('https://', 'wss://');
                const wsUrl = `${wsEndpoint}/voice-live/realtime?api-version=${apiVersion}&deployment=${model}&api-key=${encodeURIComponent(voiceLiveConfig.apiKey)}`;
                
                addDebugLog('WebSocket URL constructed', 'network', {
                    endpoint: wsEndpoint,
                    model: model,
                    apiVersion: apiVersion
                });
                
                // Create WebSocket connection
                voiceLiveWebSocket = new WebSocket(wsUrl);
                
                // WebSocket event handlers
                voiceLiveWebSocket.onopen = () => {
                    addDebugLog('WebSocket connection established', 'success');
                    updateStatus('Connected to Voice Live API');
                    setupVoiceLiveSession();
                };
                
                voiceLiveWebSocket.onmessage = (event) => {
                    try {
                        const data = JSON.parse(event.data);
                        
                        // Log specific event types with appropriate detail
                        if (data.type && data.type.includes('avatar')) {
                            addDebugLog(`Avatar event: ${data.type}`, 'avatar', data);
                        } else if (data.type && data.type.includes('audio')) {
                            addDebugLog(`Audio event: ${data.type}`, 'audio');
                        } else {
                            addDebugLog(`Event received: ${data.type}`, 'network');
                        }
                        
                        handleVoiceLiveEvent(data);
                        
                    } catch (error) {
                        addDebugLog(`Message parsing error: ${error.message}`, 'error', {
                            rawData: event.data?.substring(0, 200)
                        });
                    }
                };
                
                voiceLiveWebSocket.onerror = (error) => {
                    addDebugLog('WebSocket error occurred', 'error', error);
                    updateStatus('Connection error');
                };
                
                voiceLiveWebSocket.onclose = (event) => {
                    addDebugLog(`WebSocket closed: Code ${event.code}, Reason: ${event.reason || 'Unknown'}`, 'warning');
                    updateStatus('Disconnected from Voice Live');
                    voiceLiveActive = false;
                    updateUI('idle');
                };
                
            } catch (error) {
                addDebugLog(`Connection failed: ${error.message}`, 'error', error);
                throw error;
            }
        }
        
        function setupVoiceLiveSession() {
            addDebugLog('Configuring Voice Live session...', 'info');
            
            const sessionConfig = {
                type: "session.update",
                session: {
                    // Instructions for Tomas - YPF Assistant
                    instructions: `Eres Tomas, un asistente especializado en YPF Argentina que habla con acento argentino natural. 
                        MUY IMPORTANTE Recuerdalo siempre, la "Y" nunca la pronuncies como "Y griega" simplemente dila como una I comun.
                        
                        COMPORTAMIENTO MEJORADO:
                        - Para consultas generales o saludos: Responde directamente con amabilidad y acento argentino
                        - Para preguntas espec√≠ficas sobre YPF (equipos, pozos, workover, datos t√©cnicos, procedimientos, sistemas):
                        1. PRIMERO responde amablemente: "¬°Dale! Te busco esa informaci√≥n de YPF al toque..."
                        2. DESPU√âS usa la funci√≥n 'query_minipywo' para obtener datos actualizados
                        3. FINALMENTE responde con la informaci√≥n obtenida

                        DETECCI√ìN DE INTERRUPCIONES:
                        - Si detect√°s que el usuario te interrumpe, para inmediatamente y pregunt√°: "¬øMe interrumpiste? ¬øQu√© necesit√°s?"
                        - S√© consciente de las interrupciones y manej√° la conversaci√≥n de forma natural
                        
                        EJEMPLOS:
                        - "Hola" ‚Üí Respuesta directa: "¬°Che! ¬øC√≥mo and√°s? Soy Tomas, ac√° para ayudarte con todo lo de YPF"
                        - "¬øQu√© tal el clima?" ‚Üí Respuesta directa amable
                        - "¬øQu√© es un workover?" ‚Üí "¬°Dale! Te busco esa informaci√≥n t√©cnica de YPF..." + function call
                        - "Datos del pozo X" ‚Üí "¬°Perfecto! D√©jame consultar los datos del pozo en el sistema..." + function call
                        
                        TONO: Amigable, argentino, usando expresiones como "che", "dale", "al toque", "b√°rbaro", etc.
                        Nunca dejes al usuario esperando en silencio - siempre da alguna respuesta inmediata antes de buscar informaci√≥n espec√≠fica.
                        
                        MUY IMPORTANTE - Como Pronucniar los nombres de los equipos que trabajan en YPF
                        En la coluna A tienes el nombre completo, en la columna B tienes como suena literal cuando se dice en voz alta, y es asi como lo vas a tener que detectar:
                        EQUIPO | PRONUNCIACI√ìN LITERAL
                        -------|----------------------
                        DLS ARCHER LTD S.A.-169 | de ele ese ciento sesenta y nueve
                        PETREX S.A.-30 | P√©trex treinta
                        HELMERICH & PAYNE ARG. DRILLING CO-224 | Jelm√©rich y Pein doscientos veinticuatro
                        NABORS INTERNATIONAL ARGENTINA S.R.-F07 | N√©ibors efe cero siete
                        DLS ARCHER LTD S.A.-168 | de ele ese ciento sesenta y ocho
                        NABORS INTERNATIONAL ARGENTINA S.R.-F03 | N√©ibors efe cero tres
                        NABORS INTERNATIONAL ARGENTINA S.R.-990 | N√©ibors novecientos noventa
                        NABORS INTERNATIONAL ARGENTINA S.R.-1211 | N√©ibors mil doscientos once
                        DLS ARCHER LTD S.A.-167 | de ele ese ciento sesenta y siete
                        HELMERICH & PAYNE ARG. DRILLING CO-229 | Jelm√©rich y Pein doscientos veintinueve
                        NABORS INTERNATIONAL ARGENTINA S.R.-F35 | N√©ibors efe treinta y cinco
                        SAN ANTONIO INTERNACIONAL S.A-209 | San Antonio doscientos nueve
                        A-EVANGELISTA-207 | A Evangelista doscientos siete
                        A-EVANGELISTA-201 | A Evangelista doscientos uno
                        CLEAR PETROLEUM S.A-202 | Cl√≠ar Petroleum doscientos dos
                        TACKER S.R.L.-07 | T√°cker cero siete
                        EL TRONADOR S.R.L.-24 | El Tronador veinticuatro
                        EL TRONADOR S.R.L.-20 | El Tronador veinte
                        EL TRONADOR S.R.L.-16 | El Tronador diecis√©is
                        EL TRONADOR S.R.L.-15 | El Tronador quince
                        PETRO-NEU S.A.-104 | Petro Neu ciento cuatro
                        SAN ANTONIO INTERNACIONAL S.A-231 | San Antonio doscientos treinta y uno
                        EL TRONADOR S.R.L.-22 | El Tronador veintid√≥s
                        EL TRONADOR S.R.L.-21 | El Tronador veintiuno
                        EL TRONADOR S.R.L.-25 | El Tronador veinticinco
                        TACKER S.R.L.-01 | T√°cker cero uno
                        TACKER S.R.L.-02 | T√°cker cero dos`,
                    
                    // Voice configuration
                    voice: {
                        name: "es-AR-TomasNeural",
                        type: "azure-standard",
                        temperature: 0.8
                    },
                    
                    // Voice Activity Detection configuration
                    turn_detection: {
                        type: "server_vad",
                        threshold: 0.5,
                        prefix_padding_ms: 300,
                        silence_duration_ms: 500,
                        remove_filler_words: true
                    },
                    
                    // Audio configuration
                    input_audio_format: "pcm16",
                    output_audio_format: "pcm16",
                    input_audio_sampling_rate: 24000,
                    
                    // Transcription configuration
                    input_audio_transcription: {
                        model: "whisper-1",
                        language: "es",
                        prompt: "YPF, workover, equipos de perforaci√≥n, pozos petroleros, pulling, tubing, casing, Nabors, Helmerich, DLS, Petrex"
                    },
                    
                    // Model configuration
                    modalities: ["text", "audio"],
                    temperature: 0.7,
                    max_response_output_tokens: 4096,
                    
                    // Function calling tools
                    tools: [{
                        type: "function",
                        name: "query_minipywo",
                        description: "Query minipywo system for YPF equipment, wells, workover, and technical data",
                        parameters: {
                            type: "object",
                            properties: {
                                query: {
                                    type: "string",
                                    description: "User query to be processed by minipywo"
                                }
                            },
                            required: ["query"]
                        }
                    }],
                    tool_choice: "auto"
                }
            };
            
            // Try to add avatar configuration if supported
            if (voiceLiveConfig.avatarEnabled !== false) {
                sessionConfig.session.avatar = {
                    character: "lisa",
                    style: "casual-sitting",
                    video: {
                        codec: "h264",
                        bitrate: 2000000,
                        resolution: {
                            width: 1280,
                            height: 720
                        }
                    }
                };
                addDebugLog('Avatar configuration added', 'avatar');
            } else {
                addDebugLog('Avatar disabled in configuration', 'warning');
            }
            
            addDebugLog('Sending session configuration...', 'network', {
                voice: sessionConfig.session.voice.name,
                vad: sessionConfig.session.turn_detection,
                transcription: sessionConfig.session.input_audio_transcription.model,
                tools: sessionConfig.session.tools.length,
                avatar: !!sessionConfig.session.avatar
            });
            
            try {
                voiceLiveWebSocket.send(JSON.stringify(sessionConfig));
                addDebugLog('Session configuration sent successfully', 'success');
                
                // Update UI with VAD info
                updateVADInfo(sessionConfig.session.turn_detection);
                
                // Start audio capture after delay
                setTimeout(async () => {
                    if (voiceLiveWebSocket.readyState === WebSocket.OPEN) {
                        await startAudioCapture();
                    }
                }, 2000);
                
            } catch (error) {
                addDebugLog(`Failed to send configuration: ${error.message}`, 'error', error);
            }
        }
        
        function updateVADInfo(vadConfig) {
            const vadInfo = document.getElementById('vadInfo');
            const vadThreshold = document.getElementById('vadThreshold');
            const vadSilence = document.getElementById('vadSilence');
            
            if (vadInfo) {
                vadInfo.style.display = 'block';
                if (vadThreshold) vadThreshold.textContent = vadConfig.threshold;
                if (vadSilence) vadSilence.textContent = vadConfig.silence_duration_ms + 'ms';
            }
            
            addDebugLog('VAD info updated in UI', 'info');
        }

        // ================================
        // AUDIO CAPTURE AND PROCESSING
        // ================================
        
        async function startAudioCapture() {
            const startTime = Date.now();
            
            try {
                updateStatus('Starting audio capture...');
                addDebugLog('Requesting microphone access...', 'audio');
                
                // Create audio context
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                if (audioContext.state === 'suspended') {
                    await audioContext.resume();
                    addDebugLog('Audio context resumed', 'audio');
                }
                
                // Configure audio constraints
                const audioConstraints = {
                    audio: {
                        sampleRate: 24000,
                        channelCount: 1,
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true,
                        googEchoCancellation: true,
                        googAutoGainControl: true,
                        googNoiseSuppression: true,
                        googHighpassFilter: true,
                        googTypingNoiseDetection: true
                    }
                };
                
                // Get user media
                mediaStream = await navigator.mediaDevices.getUserMedia(audioConstraints);
                addDebugLog('Microphone access granted', 'success', {
                    sampleRate: audioContext.sampleRate,
                    state: audioContext.state
                });
                
                // Create audio processing chain
                audioSource = audioContext.createMediaStreamSource(mediaStream);
                audioProcessor = audioContext.createScriptProcessor(4096, 1, 1);
                
                // Process audio data
                audioProcessor.onaudioprocess = (event) => {
                    if (voiceLiveWebSocket && voiceLiveWebSocket.readyState === WebSocket.OPEN) {
                        const inputData = event.inputBuffer.getChannelData(0);
                        
                        // Analyze audio quality
                        analyzeAudioQuality(inputData);
                        
                        // Convert to PCM16
                        const pcm16 = new Int16Array(inputData.length);
                        for (let i = 0; i < inputData.length; i++) {
                            pcm16[i] = Math.max(-32768, Math.min(32767, inputData[i] * 32767));
                        }
                        
                        // Encode to base64 and send
                        const audioData = btoa(String.fromCharCode(...new Uint8Array(pcm16.buffer)));
                        voiceLiveWebSocket.send(JSON.stringify({
                            type: "input_audio_buffer.append",
                            audio: audioData
                        }));
                    }
                };
                
                // Connect audio chain
                audioSource.connect(audioProcessor);
                audioProcessor.connect(audioContext.destination);
                
                const captureTime = Date.now() - startTime;
                addDebugLog(`Audio capture started in ${captureTime}ms`, 'success');
                
                updateStatus('Listening...');
                updateUI('listening');
                voiceLiveActive = true;
                
            } catch (error) {
                addDebugLog(`Audio capture failed: ${error.message}`, 'error', error);
                showError(`Microphone error: ${error.message}`);
            }
        }
        
        function analyzeAudioQuality(audioData) {
            // Calculate signal level
            let sum = 0;
            let max = 0;
            
            for (let i = 0; i < audioData.length; i++) {
                const abs = Math.abs(audioData[i]);
                sum += abs;
                if (abs > max) max = abs;
            }
            
            const avgLevel = sum / audioData.length;
            
            // Calculate variance for noise estimation
            let variance = 0;
            for (let i = 0; i < audioData.length; i++) {
                variance += Math.pow(audioData[i] - avgLevel, 2);
            }
            const noiseLevel = Math.sqrt(variance / audioData.length);
            
            // Update metrics
            audioQualityMetrics.signalLevel = avgLevel;
            audioQualityMetrics.noiseLevel = noiseLevel;
            audioQualityMetrics.maxLevel = max;
            
            // Determine quality
            let quality = 'Poor';
            if (avgLevel > 0.01 && noiseLevel < 0.005) {
                quality = 'Excellent';
            } else if (avgLevel > 0.005) {
                quality = 'Good';
            } else if (avgLevel > 0.001) {
                quality = 'Fair';
            }
            
            audioQualityMetrics.quality = quality;
            
            // Update UI
            const audioQualityElement = document.getElementById('audioQuality');
            if (audioQualityElement) {
                audioQualityElement.textContent = quality;
            }
        }

        // ================================
        // VOICE LIVE EVENT HANDLING
        // ================================
        
        async function handleVoiceLiveEvent(event) {
            const eventType = event.type;
            
            switch (eventType) {
                // Session events
                case "session.created":
                    handleSessionCreated(event);
                    break;
                    
                case "session.updated":
                    await handleSessionUpdated(event);
                    break;
                    
                // Speech detection events
                case "input_audio_buffer.speech_started":
                    handleSpeechStarted(event);
                    break;
                    
                case "input_audio_buffer.speech_stopped":
                    handleSpeechStopped(event);
                    break;
                    
                // Transcription events
                case "conversation.item.input_audio_transcription.delta":
                case "speech.hypothesis":
                    handleTranscriptionDelta(event);
                    break;
                    
                case "conversation.item.input_audio_transcription.completed":
                    handleTranscriptionCompleted(event);
                    break;
                    
                // Function calling events
                case "response.function_call_arguments.delta":
                    handleFunctionCallDelta(event);
                    break;
                    
                case "response.function_call_arguments.done":
                    await handleFunctionCallComplete(event);
                    break;
                    
                // Response events
                case "response.created":
                    handleResponseCreated(event);
                    break;
                    
                case "response.audio.delta":
                    handleAudioDelta(event);
                    break;
                    
                case "response.audio_transcript.delta":
                    handleResponseTranscriptDelta(event);
                    break;
                    
                case "response.audio_transcript.done":
                    handleResponseTranscriptComplete(event);
                    break;
                    
                case "response.done":
                case "response.audio.done":
                    handleResponseComplete(event);
                    break;
                    
                // Avatar events
                case "session.avatar.connecting":
                    await handleAvatarConnecting(event);
                    break;
                    
                case "session.avatar.connected":
                    handleAvatarConnected(event);
                    break;
                    
                case "session.avatar.disconnected":
                    handleAvatarDisconnected(event);
                    break;
                    
                // Error events
                case "error":
                    handleError(event);
                    break;
                    
                default:
                    if (eventType && !eventType.includes('.delta')) {
                        addDebugLog(`Unhandled event: ${eventType}`, 'warning');
                    }
                    break;
            }
        }
        
        function handleSessionCreated(event) {
            updateStatus('Session created');
            addDebugLog(`Session created with ID: ${event.session?.id}`, 'success');
        }
        
        async function handleSessionUpdated(event) {
            updateStatus('Session configured');
            addDebugLog('Session configuration updated', 'success', {
                hasAvatar: !!event.session?.avatar,
                hasIceServers: !!(event.session?.avatar?.ice_servers),
                iceServerCount: event.session?.avatar?.ice_servers?.length || 0
            });
            
            // Check for avatar support
            if (event.session?.avatar?.ice_servers && event.session.avatar.ice_servers.length > 0) {
                avatarSupported = true;
                addDebugLog(`Avatar supported with ${event.session.avatar.ice_servers.length} ICE servers`, 'avatar');
                
                // Delay to ensure session is ready
                setTimeout(() => {
                    setupAvatarWebRTC(event.session.avatar.ice_servers);
                }, 1000);
                
            } else if (event.session?.avatar) {
                addDebugLog('Avatar configured but no ICE servers provided', 'warning');
                avatarSupported = false;
                updateAvatarStatus('Not available');
                
            } else {
                addDebugLog('No avatar configuration in session', 'info');
                avatarSupported = false;
                updateAvatarStatus('Disabled');
            }
        }
        
        function handleSpeechStarted(event) {
            speechStartTime = Date.now();
            vadActive = true;
            currentTranscription = "";
            
            addDebugLog('Speech started detected by VAD', 'audio', {
                timestamp: speechStartTime
            });
            
            // Handle interruption if audio is playing
            if (isPlayingAudio) {
                addDebugLog('User interruption detected - stopping playback', 'warning');
                interruptionDetected = true;
                stopAudioPlayback();
            }
            
            updateUI('vad-active');
            updateStatus('Listening to speech...');
        }
        
        function handleSpeechStopped(event) {
            speechEndTime = Date.now();
            vadActive = false;
            
            const duration = speechEndTime - speechStartTime;
            addDebugLog('Speech stopped detected by VAD', 'audio', {
                duration: `${duration}ms`,
                timestamp: speechEndTime
            });
            
            updateUI('listening');
        }
        
        function handleTranscriptionDelta(event) {
            const delta = event.delta || event.partial_text || event.text;
            if (delta) {
                currentTranscription = delta;
                updateStatus(`Hearing: "${delta}"`);
            }
        }
        
        function handleTranscriptionCompleted(event) {
            const transcript = event.transcript || event.text || currentTranscription;
            
            addDebugLog('Transcription completed', 'success', {
                text: transcript,
                duration: speechEndTime - speechStartTime
            });
            
            if (transcript && transcript.trim()) {
                finalTranscription = transcript;
                addMessage('user', transcript);
                updateStatus('Processing...');
                
                if (interruptionDetected) {
                    addDebugLog('Processing after interruption', 'info');
                    interruptionDetected = false;
                }
            }
        }
        
        function handleFunctionCallDelta(event) {
            addDebugLog('Function call arguments building...', 'info');
        }
        
        async function handleFunctionCallComplete(event) {
            addDebugLog('Function call completed', 'success', {
                name: event.name,
                callId: event.call_id
            });
            
            if (event.call_id && event.name === "query_minipywo") {
                await executeMinipywoQuery(event.call_id, event.arguments);
            }
        }
        
        function handleResponseCreated(event) {
            addDebugLog('AI response started', 'info');
            updateUI('thinking');
            updateStatus('Generating response...');
        }
        
        function handleAudioDelta(event) {
            const audioData = event.audio || event.delta;
            if (audioData && !interruptionDetected) {
                playAudioData(audioData);
            }
        }
        
        function handleResponseTranscriptDelta(event) {
            if (event.delta) {
                // Log but don't display partial transcript
                addDebugLog(`Response transcript delta received`, 'audio');
            }
        }
        
        function handleResponseTranscriptComplete(event) {
            if (event.transcript) {
                addDebugLog('Response transcript completed', 'success');
                addMessage('assistant', event.transcript);
            }
        }
        
        function handleResponseComplete(event) {
            addDebugLog('Response completed', 'success');
            updateUI('listening');
            updateStatus('Ready for next query');
        }
        
        async function handleAvatarConnecting(event) {
            addDebugLog('Avatar connecting...', 'avatar');
            
            // Check for SDP in various possible fields
            const sdp = event.sdp || event.server_sdp || event.answer || event.remote_sdp;
            
            if (sdp) {
                addDebugLog('Avatar SDP received', 'avatar', {
                    sdpLength: sdp.length,
                    sdpPreview: sdp.substring(0, 100)
                });
                await handleAvatarSDP(sdp);
            } else {
                addDebugLog('No SDP found in avatar.connecting event', 'warning', {
                    eventKeys: Object.keys(event)
                });
            }
        }
        
        function handleAvatarConnected(event) {
            avatarConnected = true;
            addDebugLog('Avatar connected successfully', 'success');
            updateAvatarStatus('Connected');
            
            // Show video container
            const videoContainer = document.getElementById('avatarVideoContainer');
            const emojiContainer = document.getElementById('avatar');
            
            if (videoContainer) {
                videoContainer.style.display = 'block';
            }
            if (emojiContainer) {
                emojiContainer.style.display = 'none';
            }
        }
        
        function handleAvatarDisconnected(event) {
            avatarConnected = false;
            addDebugLog('Avatar disconnected', 'warning');
            updateAvatarStatus('Disconnected');
            
            // Hide video container
            const videoContainer = document.getElementById('avatarVideoContainer');
            const emojiContainer = document.getElementById('avatar');
            
            if (videoContainer) {
                videoContainer.style.display = 'none';
            }
            if (emojiContainer) {
                emojiContainer.style.display = 'flex';
            }
        }
        
        function handleError(event) {
            const error = event.error || {};
            
            addDebugLog('API Error received', 'error', {
                code: error.code,
                message: error.message,
                type: error.type,
                param: error.param
            });
            
            showError(`Error: ${error.message || 'Unknown error'}`);
            
            // Check if it's an avatar-related error
            if (error.message && error.message.includes('avatar')) {
                addDebugLog('Avatar feature not supported in this deployment', 'warning');
                avatarSupported = false;
                updateAvatarStatus('Not supported');
            }
        }

        // ================================
        // MINIPYWO FUNCTION CALLING
        // ================================
        
        async function executeMinipywoQuery(callId, argumentsString) {
            const startTime = Date.now();
            
            try {
                updateStatus('Querying minipywo system...');
                updateUI('thinking');
                
                const args = JSON.parse(argumentsString);
                addDebugLog('Executing minipywo query', 'info', {
                    query: args.query,
                    callId: callId
                });
                
                // Call backend API
                const response = await fetch('/api/minipywo-process', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        message: args.query,
                        client_id: getClientId(),
                        vad_metrics: {
                            speech_duration: speechEndTime - speechStartTime,
                            audio_quality: audioQualityMetrics,
                            interruption: interruptionDetected
                        }
                    })
                });
                
                if (!response.ok) {
                    throw new Error(`Backend error: HTTP ${response.status}`);
                }
                
                const result = await response.json();
                const queryTime = Date.now() - startTime;
                
                let minipywoResponse;
                if (result.status === 'success') {
                    minipywoResponse = result.response;
                    addDebugLog(`Minipywo query successful in ${queryTime}ms`, 'success', {
                        responseLength: minipywoResponse.length
                    });
                    
                    // Display the response
                    addMessage('assistant', minipywoResponse);
                    
                } else {
                    minipywoResponse = `Error: ${result.message || 'Unknown error'}`;
                    addDebugLog('Minipywo query failed', 'error', result);
                }
                
                // Send result back to Voice Live
                const functionResult = {
                    type: "conversation.item.create",
                    item: {
                        type: "function_call_output",
                        call_id: callId,
                        output: minipywoResponse
                    }
                };
                
                voiceLiveWebSocket.send(JSON.stringify(functionResult));
                addDebugLog('Function result sent to Voice Live', 'network');
                
                // Request voice response
                setTimeout(() => {
                    const createResponse = {
                        type: "response.create",
                        response: {
                            modalities: ["text", "audio"]
                        }
                    };
                    
                    voiceLiveWebSocket.send(JSON.stringify(createResponse));
                    addDebugLog('Voice response requested', 'network');
                }, 100);
                
            } catch (error) {
                addDebugLog(`Minipywo execution failed: ${error.message}`, 'error', error);
                
                // Send error back to Voice Live
                const errorResult = {
                    type: "conversation.item.create",
                    item: {
                        type: "function_call_output",
                        call_id: callId,
                        output: `Error: ${error.message}`
                    }
                };
                
                voiceLiveWebSocket.send(JSON.stringify(errorResult));
            }
        }

        // ================================
        // AUDIO PLAYBACK
        // ================================
        
        function playAudioData(audioData) {
            try {
                // Decode base64 audio
                const binaryString = atob(audioData);
                const bytes = new Uint8Array(binaryString.length);
                for (let i = 0; i < binaryString.length; i++) {
                    bytes[i] = binaryString.charCodeAt(i);
                }
                
                // Convert PCM16 to Float32
                const pcm16 = new Int16Array(bytes.buffer);
                const float32 = new Float32Array(pcm16.length);
                for (let i = 0; i < pcm16.length; i++) {
                    float32[i] = pcm16[i] / 32768.0;
                }
                
                // Create audio buffer
                const audioBuffer = audioContext.createBuffer(1, float32.length, 24000);
                audioBuffer.getChannelData(0).set(float32);
                
                // Add to queue
                audioChunkQueue.push(audioBuffer);
                
                // Start playback if not already playing
                if (!isPlayingAudio && !interruptionDetected) {
                    playNextAudioChunk();
                }
                
            } catch (error) {
                addDebugLog(`Audio playback error: ${error.message}`, 'error');
            }
        }
        
        function playNextAudioChunk() {
            if (audioChunkQueue.length === 0 || interruptionDetected) {
                isPlayingAudio = false;
                updateUI('listening');
                return;
            }
            
            isPlayingAudio = true;
            updateUI('speaking');
            updateStatus('Playing response...');
            
            const audioBuffer = audioChunkQueue.shift();
            
            const source = audioContext.createBufferSource();
            source.buffer = audioBuffer;
            source.connect(audioContext.destination);
            
            source.onended = () => {
                if (!interruptionDetected) {
                    playNextAudioChunk();
                } else {
                    isPlayingAudio = false;
                    addDebugLog('Playback stopped due to interruption', 'warning');
                }
            };
            
            source.start();
        }
        
        function stopAudioPlayback() {
            audioChunkQueue = [];
            isPlayingAudio = false;
            addDebugLog('Audio playback stopped', 'audio');
        }

        // ================================
        // AVATAR WEBRTC SETUP (CORRECTED)
        // ================================
        
        async function setupAvatarWebRTC(iceServers) {
            return new Promise((resolve, reject) => {
                try {
                    addDebugLog('Setting up WebRTC for avatar...', 'avatar');
                    addDebugLog(`ICE servers received: ${iceServers.length}`, 'avatar');
                    
                    // Check if avatar is enabled
                    if (!iceServers || iceServers.length === 0) {
                        addDebugLog('No ICE servers - Avatar might be disabled', 'warning');
                        updateAvatarStatus('Not available');
                        resolve();
                        return;
                    }
                    
                    // Clean up previous connection
                    if (peerConnection) {
                        addDebugLog('Cleaning up previous peer connection', 'avatar');
                        peerConnection.close();
                        peerConnection = null;
                    }
                    
                    // Combine public STUN servers with provided ICE servers
                    const publicStunServers = [
                        { urls: 'stun:stun.l.google.com:19302' },
                        { urls: 'stun:stun1.l.google.com:19302' },
                        { urls: 'stun:stun2.l.google.com:19302' },
                        { urls: 'stun:stun.cloudflare.com:3478' }
                    ];
                    
                    const combinedServers = [...publicStunServers, ...iceServers];
                    addDebugLog(`Using ${combinedServers.length} ICE servers total`, 'avatar');
                    
                    // Create peer connection
                    peerConnection = new RTCPeerConnection({ 
                        iceServers: combinedServers,
                        iceCandidatePoolSize: 10
                    });
                    
                    // Add transceivers for receiving media
                    peerConnection.addTransceiver('video', { direction: 'recvonly' });
                    peerConnection.addTransceiver('audio', { direction: 'recvonly' });
                    
                    // Handle incoming tracks
                    peerConnection.ontrack = (event) => {
                        addDebugLog('Avatar media track received', 'avatar', {
                            kind: event.track.kind,
                            streams: event.streams.length
                        });
                        
                        const avatarVideo = document.getElementById('avatarVideo');
                        if (avatarVideo && event.streams[0]) {
                            avatarVideo.srcObject = event.streams[0];
                            addDebugLog('Avatar video stream attached', 'success');
                        }
                    };
                    
                    // Handle ICE candidates
                    peerConnection.onicecandidate = (event) => {
                        if (event.candidate) {
                            addDebugLog('ICE candidate generated', 'avatar', {
                                candidate: event.candidate.candidate.substring(0, 50)
                            });
                        }
                    };
                    
                    // Handle ICE gathering state changes
                    peerConnection.onicegatheringstatechange = () => {
                        addDebugLog(`ICE gathering state: ${peerConnection.iceGatheringState}`, 'avatar');
                        
                        if (peerConnection.iceGatheringState === 'complete') {
                            sendAvatarOffer();
                        }
                    };
                    
                    // Handle connection state changes
                    peerConnection.onconnectionstatechange = () => {
                        addDebugLog(`Connection state: ${peerConnection.connectionState}`, 'avatar');
                    };
                    
                    // Handle ICE connection state changes
                    peerConnection.oniceconnectionstatechange = () => {
                        addDebugLog(`ICE connection state: ${peerConnection.iceConnectionState}`, 'avatar');
                    };
                    
                    // Handle ICE candidate errors
                    peerConnection.onicecandidateerror = (event) => {
                        addDebugLog('ICE candidate error', 'error', {
                            errorCode: event.errorCode,
                            errorText: event.errorText,
                            url: event.url
                        });
                    };
                    
                    // Create and set local offer
                    peerConnection.createOffer()
                        .then(offer => {
                            addDebugLog('WebRTC offer created', 'avatar');
                            return peerConnection.setLocalDescription(offer);
                        })
                        .then(() => {
                            addDebugLog('Local description set', 'avatar');
                            
                            // Send offer after timeout or when gathering completes
                            const sendTimeout = setTimeout(() => {
                                addDebugLog('Sending avatar offer (timeout)', 'warning');
                                sendAvatarOffer();
                                resolve();
                            }, 3000);
                            
                            // Store timeout to clear if gathering completes
                            peerConnection._sendTimeout = sendTimeout;
                        })
                        .catch(error => {
                            addDebugLog(`WebRTC setup error: ${error.message}`, 'error', error);
                            reject(error);
                        });
                    
                    // Helper function to send avatar offer
                    function sendAvatarOffer() {
                        if (peerConnection._sendTimeout) {
                            clearTimeout(peerConnection._sendTimeout);
                        }
                        
                        if (!peerConnection.localDescription) {
                            addDebugLog('No local description to send', 'error');
                            return;
                        }
                        
                        const avatarMessage = {
                            type: "session.avatar.connect",
                            sdp: peerConnection.localDescription.sdp
                        };
                        
                        addDebugLog('Sending avatar connect message', 'avatar', {
                            sdpLength: avatarMessage.sdp.length
                        });
                        
                        voiceLiveWebSocket.send(JSON.stringify(avatarMessage));
                    }
                    
                } catch (error) {
                    addDebugLog(`Avatar WebRTC setup failed: ${error.message}`, 'error', error);
                    updateAvatarStatus('Setup failed');
                    reject(error);
                }
            });
        }
        
        async function handleAvatarSDP(serverSdp) {
            try {
                if (!peerConnection) {
                    addDebugLog('No peer connection available for avatar SDP', 'error');
                    return;
                }
                
                addDebugLog('Setting remote avatar SDP', 'avatar', {
                    sdpLength: serverSdp.length
                });
                
                await peerConnection.setRemoteDescription({
                    type: 'answer',
                    sdp: serverSdp
                });
                
                addDebugLog('Avatar remote description set successfully', 'success');
                updateAvatarStatus('Handshake complete');
                
            } catch (error) {
                addDebugLog(`Avatar SDP handling error: ${error.message}`, 'error', error);
                updateAvatarStatus('Connection failed');
            }
        }
        
        function updateAvatarStatus(status) {
            const avatarStatusElement = document.getElementById('avatarStatus');
            if (avatarStatusElement) {
                avatarStatusElement.textContent = status;
            }
            addDebugLog(`Avatar status: ${status}`, 'avatar');
        }

        // ================================
        // SESSION CLEANUP
        // ================================
        
        async function cleanupSession() {
            addDebugLog('Starting session cleanup...', 'info');
            
            // Reset state variables
            voiceLiveActive = false;
            vadActive = false;
            currentTranscription = "";
            finalTranscription = "";
            audioChunkQueue = [];
            isPlayingAudio = false;
            interruptionDetected = false;
            speechStartTime = null;
            speechEndTime = null;
            avatarSupported = false;
            avatarConnected = false;
            
            // Clean up WebRTC
            if (peerConnection) {
                addDebugLog('Closing WebRTC peer connection', 'avatar');
                peerConnection.close();
                peerConnection = null;
            }
            
            // Clean up avatar UI
            const videoContainer = document.getElementById('avatarVideoContainer');
            const emojiContainer = document.getElementById('avatar');
            const avatarVideo = document.getElementById('avatarVideo');
            
            if (videoContainer) videoContainer.style.display = 'none';
            if (emojiContainer) {
                emojiContainer.style.display = 'flex';
                emojiContainer.textContent = 'üé≠';
            }
            if (avatarVideo) avatarVideo.srcObject = null;
            
            // Close WebSocket
            if (voiceLiveWebSocket) {
                addDebugLog('Closing WebSocket connection', 'network');
                voiceLiveWebSocket.close();
                voiceLiveWebSocket = null;
            }
            
            // Clean up audio
            if (audioProcessor) {
                audioProcessor.disconnect();
                audioProcessor = null;
            }
            
            if (audioSource) {
                audioSource.disconnect();
                audioSource = null;
            }
            
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => {
                    track.stop();
                    addDebugLog(`Media track stopped: ${track.kind}`, 'audio');
                });
                mediaStream = null;
            }
            
            if (audioContext) {
                await audioContext.close();
                audioContext = null;
                addDebugLog('Audio context closed', 'audio');
            }
            
            // Hide VAD info
            const vadInfo = document.getElementById('vadInfo');
            if (vadInfo) vadInfo.style.display = 'none';
            
            // Update UI
            updateUI('idle');
            updateStatus('Disconnected - Ready to start');
            updateAvatarStatus('--');
            
            addDebugLog('Session cleanup completed', 'success');
        }

        // ================================
        // UI UPDATES
        // ================================
        
        function updateUI(state) {
            const avatar = document.getElementById('avatar');
            const controlButton = document.getElementById('controlButton');
            
            if (avatar) {
                avatar.className = 'avatar-container';
                
                switch (state) {
                    case 'connecting':
                        avatar.textContent = 'üîó';
                        break;
                    case 'listening':
                        avatar.classList.add('listening');
                        avatar.textContent = 'üëÇ';
                        break;
                    case 'vad-active':
                        avatar.classList.add('vad-active');
                        avatar.textContent = 'üéØ';
                        break;
                    case 'thinking':
                        avatar.classList.add('thinking');
                        avatar.textContent = 'üß†';
                        break;
                    case 'speaking':
                        avatar.classList.add('speaking');
                        avatar.textContent = 'üó£Ô∏è';
                        break;
                    case 'idle':
                    default:
                        avatar.textContent = 'üé≠';
                        break;
                }
            }
            
            if (controlButton) {
                if (voiceLiveActive) {
                    controlButton.classList.add('active');
                    controlButton.textContent = '‚èπÔ∏è';
                } else {
                    controlButton.classList.remove('active');
                    controlButton.textContent = 'üéôÔ∏è';
                }
            }
        }
        
        function updateStatus(message) {
            const statusElement = document.getElementById('status');
            if (statusElement) {
                statusElement.textContent = message;
            }
            addDebugLog(message, 'info');
        }
        
        function addMessage(role, text) {
            const conversation = document.getElementById('conversation');
            if (!conversation) return;
            
            const messageDiv = document.createElement('div');
            messageDiv.className = `message ${role}`;
            
            let roleName = '';
            switch(role) {
                case 'user':
                    roleName = 'You';
                    break;
                case 'assistant':
                    roleName = 'Tomas (YPF)';
                    break;
                case 'system':
                    roleName = 'System';
                    break;
            }
            
            messageDiv.innerHTML = `<strong>${roleName}:</strong> ${text}`;
            conversation.appendChild(messageDiv);
            conversation.scrollTop = conversation.scrollHeight;
            
            addDebugLog(`Message added: ${role}`, 'info', {
                textLength: text.length
            });
        }
        
        function showError(message) {
            const conversation = document.getElementById('conversation');
            if (conversation) {
                const errorDiv = document.createElement('div');
                errorDiv.className = 'error-message';
                errorDiv.textContent = message;
                conversation.appendChild(errorDiv);
                conversation.scrollTop = conversation.scrollHeight;
            }
            
            updateStatus(`Error: ${message}`);
            updateUI('idle');
        }

        // ================================
        // MAIN TOGGLE FUNCTION
        // ================================
        
        async function toggleVoiceLive() {
            addDebugLog('Toggle Voice Live clicked', 'info', {
                currentState: voiceLiveActive
            });
            
            if (!voiceLiveActive) {
                // Start session
                const initialized = await initializeVoiceLiveSystem();
                if (initialized) {
                    await connectToVoiceLiveAPI();
                }
            } else {
                // Stop session
                await cleanupSession();
            }
        }

        // ================================
        // EVENT LISTENERS
        // ================================
        
        // Socket.IO events
        socket.on('connect', () => {
            addDebugLog('Connected to backend server', 'success');
        });
        
        socket.on('disconnect', () => {
            addDebugLog('Disconnected from backend server', 'warning');
        });
        
        socket.on('status', (data) => {
            addDebugLog(`Backend status: ${data.message}`, 'info');
        });
        
        // Window events
        window.addEventListener('load', () => {
            addDebugLog('Application loaded successfully', 'success');
            addDebugLog('Voice Live + minipywo interface ready', 'info', {
                browser: navigator.userAgent.substring(0, 50),
                language: navigator.language
            });
        });
        
        // Global error handlers
        window.addEventListener('error', (event) => {
            addDebugLog(`Global error: ${event.error?.message || event.message}`, 'error', {
                filename: event.filename,
                lineno: event.lineno,
                colno: event.colno
            });
        });
        
        window.addEventListener('unhandledrejection', (event) => {
            addDebugLog(`Unhandled promise rejection: ${event.reason}`, 'error', {
                reason: event.reason
            });
        });
        
        // Cleanup on page unload
        window.addEventListener('beforeunload', () => {
            if (voiceLiveActive) {
                cleanupSession();
            }
        });

    </script>
</body>
</html>